{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5826ee18",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/harpreetsahota204/medgemma_kaggle_competition/blob/main/medgemma_impact_starter.ipynb)\n",
    "\n",
    "# Winning the MedGemma Impact Challenge with FiftyOne\n",
    "\n",
    "**The difference between a demo and a winning submission is understanding where your model breaks—and why.**\n",
    "\n",
    "This notebook shows you how to use [FiftyOne](https://docs.voxel51.com/) as your workbench for the MedGemma Impact Challenge. \n",
    "We'll go beyond running inference and printing metrics. You'll learn to:\n",
    "\n",
    "1. **Explore your data** before modeling\n",
    "2. **Visualize embeddings** to diagnose learnability  \n",
    "3. **Run MedGemma inference** and store predictions alongside ground truth\n",
    "4. **Analyze failures** systematically—not just count them\n",
    "5. **Fine-tune for localization** using FiftyOne's PyTorch integration\n",
    "\n",
    "We'll use the [SLAKE dataset](https://huggingface.co/datasets/Voxel51/SLAKE), a medical VQA benchmark \n",
    "with images from multiple modalities (CT, MRI, X-ray), rich annotations including bounding boxes and \n",
    "segmentation masks, and questions spanning anatomy, abnormalities, and more.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f184fdcf",
   "metadata": {},
   "source": [
    "## Setup & Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7db1955",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U fiftyone huggingface_hub accelerate sentencepiece protobuf torch torchvision umap-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f6e801",
   "metadata": {},
   "source": [
    "### Authenticate with Hugging Face\n",
    "\n",
    "Both MedGemma and MedSigLIP are gated models. You'll need to:\n",
    "1. Request access on [MedGemma](https://huggingface.co/google/medgemma-1.5-4b-it) and [MedSigLIP](https://huggingface.co/google/medsiglip-448)\n",
    "2. Set your HF token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe350ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# os.environ[\"HF_TOKEN\"] = \"your_token_here\"\n",
    "\n",
    "# Or login via CLI: hf auth login\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" #update for your setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45462b53",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Load the SLAKE Dataset\n",
    "\n",
    "The SLAKE dataset is already in [FiftyOne format](https://docs.voxel51.com/user_guide/using_datasets.html) on Hugging Face. \n",
    "One line to load it using the [`load_from_hub()`](https://docs.voxel51.com/api/fiftyone.utils.huggingface.html#fiftyone.utils.huggingface.load_from_hub) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6dad7b4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading config file fiftyone.yml from Voxel51/SLAKE\n",
      "Loading dataset\n",
      "Importing samples...\n",
      " 100% |███████████████████| 50/50 [3.8ms elapsed, 0s remaining, 13.1K samples/s]      \n"
     ]
    }
   ],
   "source": [
    "from fiftyone.utils.huggingface import load_from_hub\n",
    "\n",
    "dataset = load_from_hub(\n",
    "    \"Voxel51/SLAKE\",\n",
    "    name=\"SLAKE\",\n",
    "    overwrite=True,\n",
    "    max_samples=50 #taking a small subset of the dataset for this example\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49186060",
   "metadata": {},
   "source": [
    "### Understanding FiftyOne Datasets\n",
    "\n",
    "A FiftyOne [Dataset](https://docs.voxel51.com/api/fiftyone.core.dataset.html#fiftyone.core.dataset.Dataset) \n",
    "is comprised of [Samples](https://docs.voxel51.com/api/fiftyone.core.sample.html#fiftyone.core.sample.Sample).\n",
    "\n",
    "**Samples** store all information associated with a particular piece of data in a dataset, including:\n",
    "- Basic metadata about the data\n",
    "- One or more sets of labels\n",
    "- Additional features associated with subsets of the data and/or label sets\n",
    "\n",
    "The attributes of a Sample are called [Fields](https://docs.voxel51.com/api/fiftyone.core.fields.html#fiftyone.core.fields.Field), \n",
    "which store information about the Sample. When a new Field is assigned to a Sample in a Dataset, \n",
    "it is automatically added to the dataset's schema and thus accessible on all other samples in the dataset.\n",
    "\n",
    "Let's look at the schema to understand what we're working with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53e3ac8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name:        SLAKE\n",
       "Media type:  image\n",
       "Num samples: 50\n",
       "Persistent:  False\n",
       "Tags:        []\n",
       "Sample fields:\n",
       "    id:               fiftyone.core.fields.ObjectIdField\n",
       "    filepath:         fiftyone.core.fields.StringField\n",
       "    tags:             fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n",
       "    metadata:         fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.ImageMetadata)\n",
       "    created_at:       fiftyone.core.fields.DateTimeField\n",
       "    last_modified_at: fiftyone.core.fields.DateTimeField\n",
       "    detections:       fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detections)\n",
       "    segmentation:     fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Segmentation)\n",
       "    location:         fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Classification)\n",
       "    modality:         fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Classification)\n",
       "    base_type:        fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Classification)\n",
       "    answer_type:      fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Classification)\n",
       "    question_0:       fiftyone.core.fields.StringField\n",
       "    answer_0:         fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Classification)\n",
       "    question_1:       fiftyone.core.fields.StringField\n",
       "    answer_1:         fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Classification)\n",
       "    question_2:       fiftyone.core.fields.StringField\n",
       "    answer_2:         fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Classification)\n",
       "    question_3:       fiftyone.core.fields.StringField\n",
       "    answer_3:         fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Classification)\n",
       "    question_4:       fiftyone.core.fields.StringField\n",
       "    answer_4:         fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Classification)\n",
       "    question_5:       fiftyone.core.fields.StringField\n",
       "    answer_5:         fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Classification)\n",
       "    question_6:       fiftyone.core.fields.StringField\n",
       "    answer_6:         fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Classification)\n",
       "    question_7:       fiftyone.core.fields.StringField\n",
       "    answer_7:         fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Classification)\n",
       "    question_8:       fiftyone.core.fields.StringField\n",
       "    answer_8:         fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Classification)\n",
       "    question_9:       fiftyone.core.fields.StringField\n",
       "    answer_9:         fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Classification)\n",
       "    question_10:      fiftyone.core.fields.StringField\n",
       "    answer_10:        fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Classification)\n",
       "    question_11:      fiftyone.core.fields.StringField\n",
       "    answer_11:        fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Classification)\n",
       "    question_12:      fiftyone.core.fields.StringField\n",
       "    answer_12:        fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Classification)\n",
       "    question_13:      fiftyone.core.fields.StringField\n",
       "    answer_13:        fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Classification)\n",
       "    question_14:      fiftyone.core.fields.StringField\n",
       "    answer_14:        fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Classification)\n",
       "    question_15:      fiftyone.core.fields.StringField\n",
       "    answer_15:        fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Classification)\n",
       "    question_16:      fiftyone.core.fields.StringField\n",
       "    answer_16:        fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Classification)\n",
       "    question_17:      fiftyone.core.fields.StringField\n",
       "    answer_17:        fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Classification)\n",
       "    question_18:      fiftyone.core.fields.StringField\n",
       "    answer_18:        fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Classification)\n",
       "    question_19:      fiftyone.core.fields.StringField\n",
       "    answer_19:        fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Classification)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d305f0e",
   "metadata": {},
   "source": [
    "To see the contents of a single Sample and its Fields, you can use the \n",
    "[`first()` method](https://docs.voxel51.com/api/fiftyone.core.dataset.html#fiftyone.core.dataset.Dataset.first):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96aed697",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Sample: {\n",
       "    'id': '6830a40b7a3316c43716ba5b',\n",
       "    'media_type': 'image',\n",
       "    'filepath': '/home/harpreet/fiftyone/huggingface/hub/Voxel51/SLAKE/data/source_xmlab132.jpg',\n",
       "    'tags': [],\n",
       "    'metadata': <ImageMetadata: {\n",
       "        'size_bytes': 179352,\n",
       "        'mime_type': 'image/jpeg',\n",
       "        'width': 1024,\n",
       "        'height': 1024,\n",
       "        'num_channels': 3,\n",
       "    }>,\n",
       "    'created_at': datetime.datetime(2026, 1, 19, 21, 42, 13, 156000),\n",
       "    'last_modified_at': datetime.datetime(2026, 1, 19, 21, 42, 13, 156000),\n",
       "    'detections': <Detections: {\n",
       "        'detections': [\n",
       "            <Detection: {\n",
       "                'id': '6830a40a7a3316c437168c02',\n",
       "                'attributes': {},\n",
       "                'tags': [],\n",
       "                'label': 'Cardiomegaly',\n",
       "                'bounding_box': [0.3779296875, 0.4091796875, 0.490234375, 0.3583984375],\n",
       "                'mask': None,\n",
       "                'mask_path': None,\n",
       "                'confidence': None,\n",
       "                'index': None,\n",
       "            }>,\n",
       "        ],\n",
       "    }>,\n",
       "    'segmentation': <Segmentation: {\n",
       "        'id': '6830a40a7a3316c437168c03',\n",
       "        'tags': [],\n",
       "        'mask': None,\n",
       "        'mask_path': '/home/harpreet/fiftyone/huggingface/hub/Voxel51/SLAKE/fields/segmentation/mask_xmlab132.png',\n",
       "    }>,\n",
       "    'location': <Classification: {\n",
       "        'id': '6830a40a7a3316c437168c04',\n",
       "        'tags': [],\n",
       "        'label': 'Lung',\n",
       "        'confidence': None,\n",
       "        'logits': None,\n",
       "    }>,\n",
       "    'modality': <Classification: {\n",
       "        'id': '6830a40a7a3316c437168c05',\n",
       "        'tags': [],\n",
       "        'label': 'X-Ray',\n",
       "        'confidence': None,\n",
       "        'logits': None,\n",
       "    }>,\n",
       "    'base_type': <Classification: {\n",
       "        'id': '6830a40a7a3316c437168c06',\n",
       "        'tags': [],\n",
       "        'label': 'vqa',\n",
       "        'confidence': None,\n",
       "        'logits': None,\n",
       "    }>,\n",
       "    'answer_type': <Classification: {\n",
       "        'id': '6830a40a7a3316c437168c07',\n",
       "        'tags': [],\n",
       "        'label': 'OPEN',\n",
       "        'confidence': None,\n",
       "        'logits': None,\n",
       "    }>,\n",
       "    'question_0': 'What modality is used to take this image?',\n",
       "    'answer_0': <Classification: {\n",
       "        'id': '6830a40a7a3316c437168c08',\n",
       "        'tags': [],\n",
       "        'label': 'X-Ray',\n",
       "        'confidence': None,\n",
       "        'logits': None,\n",
       "    }>,\n",
       "    'question_1': 'Which part of the body does this image belong to?',\n",
       "    'answer_1': <Classification: {\n",
       "        'id': '6830a40a7a3316c437168c09',\n",
       "        'tags': [],\n",
       "        'label': 'Chest',\n",
       "        'confidence': None,\n",
       "        'logits': None,\n",
       "    }>,\n",
       "    'question_2': 'Are there abnormalities in this image?',\n",
       "    'answer_2': <Classification: {\n",
       "        'id': '6830a40a7a3316c437168c0a',\n",
       "        'tags': [],\n",
       "        'label': 'Yes',\n",
       "        'confidence': None,\n",
       "        'logits': None,\n",
       "    }>,\n",
       "    'question_3': 'What is the largest organ in the picture?',\n",
       "    'answer_3': <Classification: {\n",
       "        'id': '6830a40a7a3316c437168c0b',\n",
       "        'tags': [],\n",
       "        'label': 'Lung',\n",
       "        'confidence': None,\n",
       "        'logits': None,\n",
       "    }>,\n",
       "    'question_4': 'What diseases are included in the picture?',\n",
       "    'answer_4': <Classification: {\n",
       "        'id': '6830a40a7a3316c437168c0c',\n",
       "        'tags': [],\n",
       "        'label': 'Cardiomegal',\n",
       "        'confidence': None,\n",
       "        'logits': None,\n",
       "    }>,\n",
       "    'question_5': 'Where is/are the abnormality located?',\n",
       "    'answer_5': <Classification: {\n",
       "        'id': '6830a40a7a3316c437168c0d',\n",
       "        'tags': [],\n",
       "        'label': 'Center',\n",
       "        'confidence': None,\n",
       "        'logits': None,\n",
       "    }>,\n",
       "    'question_6': 'Which organ is abnormal, heart or lung?',\n",
       "    'answer_6': <Classification: {\n",
       "        'id': '6830a40a7a3316c437168c0e',\n",
       "        'tags': [],\n",
       "        'label': 'Heart',\n",
       "        'confidence': None,\n",
       "        'logits': None,\n",
       "    }>,\n",
       "    'question_7': None,\n",
       "    'answer_7': None,\n",
       "    'question_8': None,\n",
       "    'answer_8': None,\n",
       "    'question_9': None,\n",
       "    'answer_9': None,\n",
       "    'question_10': None,\n",
       "    'answer_10': None,\n",
       "    'question_11': None,\n",
       "    'answer_11': None,\n",
       "    'question_12': None,\n",
       "    'answer_12': None,\n",
       "    'question_13': None,\n",
       "    'answer_13': None,\n",
       "    'question_14': None,\n",
       "    'answer_14': None,\n",
       "    'question_15': None,\n",
       "    'answer_15': None,\n",
       "    'question_16': None,\n",
       "    'answer_16': None,\n",
       "    'question_17': None,\n",
       "    'answer_17': None,\n",
       "    'question_18': None,\n",
       "    'answer_18': None,\n",
       "    'question_19': None,\n",
       "    'answer_19': None,\n",
       "}>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59a1208",
   "metadata": {},
   "source": [
    "### Understanding the SLAKE Schema\n",
    "\n",
    "This dataset is **image-centric**: each of the 642 samples represents one medical image,\n",
    "with multiple Q&A pairs attached to it. Let's break down the key fields:\n",
    "\n",
    "**Metadata fields** (stored as `Classification` objects—access via `.label`):\n",
    "- `modality`: Imaging modality (CT, MRI, X-Ray) \n",
    "- `location`: Anatomical region (Lung, Brain, Abdomen, etc.)\n",
    "- `answer_type`: Question type (OPEN or CLOSED)\n",
    "- `base_type`: Task type (vqa)\n",
    "\n",
    "**Multiple Q&A pairs** (up to 20 per image):\n",
    "- `question_0`, `question_1`, ... `question_19`: Question strings\n",
    "- `answer_0`, `answer_1`, ... `answer_19`: Answer as `Classification` objects\n",
    "\n",
    "**Annotations** (where available):\n",
    "- `detections`: Bounding boxes with labels (e.g., \"Cardiomegaly\")\n",
    "- `segmentation`: Segmentation masks with `mask_path`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eee2265",
   "metadata": {},
   "source": [
    "### Accessing Classification Fields\n",
    "\n",
    "Many fields in this dataset are stored as FiftyOne \n",
    "[`Classification`](https://docs.voxel51.com/api/fiftyone.core.labels.html#fiftyone.core.labels.Classification) \n",
    "objects. To get the actual value, access the `.label` attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "719f4777",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modality: X-Ray\n",
      "Location: Lung\n",
      "Answer Type: OPEN\n",
      "\n",
      "Question 0: What modality is used to take this image?\n",
      "Answer 0: X-Ray\n"
     ]
    }
   ],
   "source": [
    "sample = dataset.first()\n",
    "\n",
    "# These are Classification objects - access .label to get the string value\n",
    "print(f\"Modality: {sample.modality.label}\")\n",
    "print(f\"Location: {sample.location.label}\")\n",
    "print(f\"Answer Type: {sample.answer_type.label}\")\n",
    "\n",
    "# Questions are stored as strings\n",
    "print(f\"\\nQuestion 0: {sample.question_0}\")\n",
    "\n",
    "# Answers are Classification objects\n",
    "print(f\"Answer 0: {sample.answer_0.label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4c8d80",
   "metadata": {},
   "source": [
    "### Slicing Field Values with `ViewField`\n",
    "\n",
    "**Key Concept:** Methods like `count_values(\"modality.label\")` work because they accept \n",
    "**field paths as strings** (using dot notation). However, **slicing/indexing requires \n",
    "`ViewField` expressions**.\n",
    "\n",
    "**String field paths** (dot notation) work for:\n",
    "- `count_values(\"modality.label\")`\n",
    "- `distinct(\"modality.label\")`\n",
    "- `sort_by(\"modality.label\")`\n",
    "\n",
    "**`ViewField` expressions** are required for:\n",
    "- Array indexing: `F(\"bounding_box\")[2]`\n",
    "- Array slicing: `F(\"detections\")[1:3]`\n",
    "- String slicing: `F(\"text_field\")[:10]`\n",
    "\n",
    "```python\n",
    "from fiftyone import ViewField as F\n",
    "\n",
    "# ❌ This won't work (can't slice string paths)\n",
    "dataset.count_values(\"predictions.detections[0].label\")\n",
    "\n",
    "# ✅ Use ViewField for slicing\n",
    "expr = F(\"predictions.detections\")[0].label\n",
    "dataset.count_values(expr)\n",
    "\n",
    "# ✅ Other examples\n",
    "bbox_width = F(\"bounding_box\")[2]\n",
    "first_three = F(\"detections\")[:3]\n",
    "```\n",
    "\n",
    "**Summary:**\n",
    "- **Dot notation strings** = simple field paths\n",
    "- **`F(...)` expressions** = when you need indexing/slicing operations on field values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6b18e9",
   "metadata": {},
   "source": [
    "### Exploring Q&A Pairs\n",
    "\n",
    "Each image has multiple question-answer pairs. Let's look at a single sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4b70ec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Q&A pairs:\n",
      "\n",
      "Q0: What modality is used to take this image?\n",
      "A0: X-Ray\n",
      "\n",
      "Q1: Which part of the body does this image belong to?\n",
      "A1: Chest\n",
      "\n",
      "Q2: Are there abnormalities in this image?\n",
      "A2: Yes\n",
      "\n",
      "Q3: What is the largest organ in the picture?\n",
      "A3: Lung\n",
      "\n",
      "Q4: What diseases are included in the picture?\n",
      "A4: Cardiomegal\n",
      "\n",
      "Q5: Where is/are the abnormality located?\n",
      "A5: Center\n",
      "\n",
      "Q6: Which organ is abnormal, heart or lung?\n",
      "A6: Heart\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample = dataset.first()\n",
    "\n",
    "# Print Q&A pairs for this sample\n",
    "print(f\"Sample Q&A pairs:\\n\")\n",
    "for i in range(7):  # First 7 questions (most samples have ~7)\n",
    "    q = getattr(sample, f\"question_{i}\")\n",
    "    a = getattr(sample, f\"answer_{i}\")\n",
    "    if q is not None:\n",
    "        print(f\"Q{i}: {q}\")\n",
    "        print(f\"A{i}: {a.label if a else 'None'}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d9865d",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Explore Your Data (Before You Model)\n",
    "\n",
    "Don't rush to inference. Understanding your data distribution is how you catch problems early.\n",
    "\n",
    "FiftyOne provides powerful functionality to compute statistics about your dataset using \n",
    "[built-in Aggregation methods](https://docs.voxel51.com/user_guide/using_aggregations.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce77df1b",
   "metadata": {},
   "source": [
    "### What modalities do we have?\n",
    "\n",
    "Use the [`count_values()` aggregation](https://docs.voxel51.com/api/fiftyone.core.collections.html#fiftyone.core.collections.SampleCollection.count_values) \n",
    "to compute the occurrences of field values in a collection.\n",
    "\n",
    "**Important:** Since `modality` is a Classification field, we need to access \n",
    "the `.label` attribute using dot notation in the field path:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5fe4b6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'X-Ray': 12, 'CT': 17, 'MRI': 21}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.count_values(\"modality.label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1ee31d",
   "metadata": {},
   "source": [
    "### What anatomical locations are covered?\n",
    "\n",
    "The `location` field tells us what body part/organ the image focuses on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42cd168f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Brain_Tissue': 15,\n",
       " 'Lung': 14,\n",
       " 'Neck': 1,\n",
       " 'Pelvic Cavity': 2,\n",
       " 'Brain': 2,\n",
       " 'Chest_lung': 3,\n",
       " 'Abdomen': 13}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.count_values(\"location.label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be3f5c6",
   "metadata": {},
   "source": [
    "### What types of questions?\n",
    "\n",
    "The `answer_type` field indicates whether questions are OPEN (free-form) or CLOSED (yes/no, multiple choice):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d5c5c740",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'OPEN': 28, 'CLOSED': 22}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.count_values(\"answer_type.label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b0f0101",
   "metadata": {},
   "source": [
    "### What detection labels exist?\n",
    "\n",
    "The `detections` field contains bounding boxes with labels (e.g., anatomical structures, \n",
    "abnormalities). Use [`count_values()`](https://docs.voxel51.com/api/fiftyone.core.collections.html#fiftyone.core.collections.SampleCollection.count_values) \n",
    "on nested fields:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a6afddbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Left Eye': 1,\n",
       " 'Spinal Cord': 9,\n",
       " 'Left Kidney': 4,\n",
       " 'Left Temporal Lobe': 1,\n",
       " 'Esophagus': 2,\n",
       " 'Nodule': 3,\n",
       " 'Spleen': 7,\n",
       " 'Mass': 3,\n",
       " 'Liver': 14,\n",
       " 'Stomach': 1,\n",
       " 'Left Lung': 4,\n",
       " 'Small Bowel': 9,\n",
       " 'Brain Non-enhancing Tumor': 17,\n",
       " 'Right Temporal Lobe': 1,\n",
       " 'Trachea': 1,\n",
       " 'Pneumonia': 3,\n",
       " 'Right Lung': 5,\n",
       " 'Brain Enhancing Tumor': 10,\n",
       " 'Right Eye': 1,\n",
       " 'Cardiomegaly': 3,\n",
       " 'Heart': 3,\n",
       " 'Rectum': 1,\n",
       " 'Bladder': 1,\n",
       " 'Brain Edema': 20,\n",
       " 'Duodenum': 2,\n",
       " 'Right Kidney': 3,\n",
       " 'Colon': 6}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.count_values(\"detections.detections.label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abdf9c10",
   "metadata": {},
   "source": [
    "### Launch the App to explore visually\n",
    "\n",
    "The most powerful part of FiftyOne is [the FiftyOne App](https://docs.voxel51.com/user_guide/app.html#using-the-fiftyone-app), \n",
    "which runs locally on your machine. Filter, sort, and browse your data interactively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa2fdc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "session = fo.launch_app(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07995739",
   "metadata": {},
   "source": [
    "# ![Explore MedGemma](https://raw.githubusercontent.com/harpreetsahota204/medgemma_kaggle_competition/main/gifs/explore_med_gemma.gif)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c205a3",
   "metadata": {},
   "source": [
    "**Try these in the App:**\n",
    "- In sidebar of the app, under the Labels section, click the dropdown for `modality` and click the check box for CT to filter the samples in the panel to only CT scans\n",
    "- Try the same for the to `location` label, for example filter to \"Lung\"` to see lung images\n",
    "- Look at samples with detections (bounding boxes) vs without\n",
    "- Explore the Q&A pairs in the sample panel\n",
    "\n",
    "You'll start to notice patterns: certain anatomical locations have more images, \n",
    "certain modalities are over/under-represented, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc486c0",
   "metadata": {},
   "source": [
    "### Create useful Dataset Views\n",
    "\n",
    "[Dataset Views](https://docs.voxel51.com/user_guide/using_views.html) let you filter, sort, and \n",
    "slice your data without modifying the underlying dataset. Views are powerful because they:\n",
    "- Chain multiple operations together\n",
    "- Are lazily evaluated for efficiency\n",
    "- Can be saved and reloaded\n",
    "\n",
    "You can use [`ViewField`](https://docs.voxel51.com/api/fiftyone.core.expressions.html#fiftyone.core.expressions.ViewField) \n",
    "and [`ViewExpression`](https://docs.voxel51.com/api/fiftyone.core.expressions.html#fiftyone.core.expressions.ViewExpression) \n",
    "classes to define expressions using native Python operators. Simply wrap the target field in a \n",
    "`ViewField` and apply comparison, logic, arithmetic or array operations to it.\n",
    "\n",
    "Learn more about [creating Views](https://docs.voxel51.com/cheat_sheets/views_cheat_sheet.html) \n",
    "and [filtering](https://docs.voxel51.com/cheat_sheets/filtering_cheat_sheet.html) in the cheat sheets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cd5eef03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images with CLOSED questions: 22\n",
      "Images with detections: 47\n",
      "X-Ray images: 12\n",
      "CT images: 17\n",
      "Lung images: 14\n"
     ]
    }
   ],
   "source": [
    "from fiftyone import ViewField as F\n",
    "# CLOSED answer type only (yes/no questions - easier to evaluate)\n",
    "# Note: Use \"answer_type.label\" to filter on the Classification's label\n",
    "closed_questions = dataset.match(F(\"answer_type.label\") == \"CLOSED\")\n",
    "dataset.save_view(\"closed_questions\", closed_questions)\n",
    "print(f\"Images with CLOSED questions: {len(closed_questions)}\")\n",
    "\n",
    "# Images with detection annotations (bounding boxes)\n",
    "has_detections = dataset.match(F(\"detections.detections\").length() > 0)\n",
    "dataset.save_view(\"has_detections\", has_detections)\n",
    "print(f\"Images with detections: {len(has_detections)}\")\n",
    "\n",
    "# X-Ray images only\n",
    "xray_only = dataset.match(F(\"modality.label\") == \"X-Ray\")\n",
    "dataset.save_view(\"xray_only\", xray_only)\n",
    "print(f\"X-Ray images: {len(xray_only)}\")\n",
    "\n",
    "# CT images only\n",
    "ct_only = dataset.match(F(\"modality.label\") == \"CT\")\n",
    "dataset.save_view(\"ct_only\", ct_only)\n",
    "print(f\"CT images: {len(ct_only)}\")\n",
    "\n",
    "# Lung images\n",
    "lung_images = dataset.match(F(\"location.label\") == \"Lung\")\n",
    "dataset.save_view(\"lung_images\", lung_images)\n",
    "print(f\"Lung images: {len(lung_images)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556936b7",
   "metadata": {},
   "source": [
    "For those familiar with `pandas`, check out this \n",
    "[pandas vs FiftyOne cheat sheet](https://docs.voxel51.com/cheat_sheets/pandas_vs_fiftyone.html) \n",
    "to learn how to translate common pandas operations into FiftyOne syntax."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267f7220",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Compute Embeddings with MedSigLIP\n",
    "\n",
    "Before running VQA inference, let's see if the embedding space even separates our classes.\n",
    "If MedSigLIP embeddings don't cluster by modality or body part, that's diagnostic information.\n",
    "\n",
    "You can visualize [image embeddings](https://docs.voxel51.com/brain.html#visualizing-embeddings) \n",
    "using models from the [FiftyOne Model Zoo](https://docs.voxel51.com/model_zoo/overview.html), \n",
    "or custom models which you can integrate as a [Remote Zoo Model](https://docs.voxel51.com/model_zoo/remote.html#remotely-sourced-zoo-models)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27efb5f",
   "metadata": {},
   "source": [
    "### Register and load MedSigLIP\n",
    "\n",
    "MedSigLIP is available as a Remote Zoo Model. First, register the model source:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bb452f84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://github.com/harpreetsahota204/medsiglip...\n",
      "   22.2Mb [463.1ms elapsed, ? remaining, 47.9Mb/s] \n",
      "Overwriting existing model source '/home/harpreet/fiftyone/__models__/medsiglip'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01070d13c5cb4e1a96ac009f97242859",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (incomplete total...): 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b7e343660244cd3a0e8da3a4b97c379",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 9 files:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(<fiftyone.zoo.models.RemoteZooModel at 0x7625c032ba10>,\n",
       " '/home/harpreet/fiftyone/__models__/medsiglip/medsiglip-448')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import fiftyone.zoo as foz\n",
    "# Register the model source (one time)\n",
    "foz.register_zoo_model_source(\n",
    "    \"https://github.com/harpreetsahota204/medsiglip\",\n",
    "    overwrite=True\n",
    ")\n",
    "\n",
    "# Download the model (one time)\n",
    "foz.download_zoo_model(\n",
    "    \"https://github.com/harpreetsahota204/medsiglip\",\n",
    "    model_name=\"google/medsiglip-448\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6c979196",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "928620e3394540758ab991d62fe2b6bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (incomplete total...): 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "559c6f15f5bf4588b0ceba48f80ff84b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 9 files:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3947bf4355d4b93bf6c48a6939107fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/888 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the model\n",
    "medsiglip = foz.load_zoo_model(\"google/medsiglip-448\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23cd9e2c",
   "metadata": {},
   "source": [
    "### Compute embeddings\n",
    "\n",
    "Use the [`compute_embeddings()` method](https://docs.voxel51.com/api/fiftyone.core.collections.html#fiftyone.core.collections.SampleCollection.compute_embeddings) \n",
    "to compute embeddings for all samples in your dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "06e3f16b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% |███████████████████| 50/50 [3.9s elapsed, 0s remaining, 19.9 samples/s]      \n"
     ]
    }
   ],
   "source": [
    "dataset.compute_embeddings(\n",
    "    model=medsiglip,\n",
    "    embeddings_field=\"medsiglip_embeddings\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5e4006",
   "metadata": {},
   "source": [
    "### Visualize in 2D\n",
    "\n",
    "Use the [`compute_visualization()` method](https://docs.voxel51.com/api/fiftyone.brain.html#fiftyone.brain.compute_visualization) \n",
    "to generate low-dimensional representations of the samples in your Dataset. \n",
    "This projects high-dimensional embeddings to 2D/3D for visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7976e71f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating visualization...\n",
      "UMAP( verbose=True)\n",
      "Mon Jan 19 15:43:22 2026 Construct fuzzy simplicial set\n",
      "Mon Jan 19 15:43:23 2026 Finding Nearest Neighbors\n",
      "Mon Jan 19 15:43:26 2026 Finished Nearest Neighbor Search\n",
      "Mon Jan 19 15:43:28 2026 Construct embedding\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab6db928c0494952a9789dd4016d6331",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs completed:   0%|            0/500 [00:00]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tcompleted  0  /  500 epochs\n",
      "\tcompleted  50  /  500 epochs\n",
      "\tcompleted  100  /  500 epochs\n",
      "\tcompleted  150  /  500 epochs\n",
      "\tcompleted  200  /  500 epochs\n",
      "\tcompleted  250  /  500 epochs\n",
      "\tcompleted  300  /  500 epochs\n",
      "\tcompleted  350  /  500 epochs\n",
      "\tcompleted  400  /  500 epochs\n",
      "\tcompleted  450  /  500 epochs\n",
      "Mon Jan 19 15:43:30 2026 Finished embedding\n"
     ]
    }
   ],
   "source": [
    "import fiftyone.brain as fob\n",
    "\n",
    "results = fob.compute_visualization(\n",
    "    dataset,\n",
    "    embeddings=\"medsiglip_embeddings\",\n",
    "    method=\"umap\",\n",
    "    brain_key=\"medsiglip_viz\",\n",
    "    num_dims=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d47c157",
   "metadata": {},
   "source": [
    "### Build a similarity index for later\n",
    "\n",
    "Use the [`compute_similarity()` method](https://docs.voxel51.com/api/fiftyone.brain.html#fiftyone.brain.compute_similarity) \n",
    "to build a similarity index over the images in your dataset. This allows you to \n",
    "[sort by similarity](https://docs.voxel51.com/brain.html#sorting-by-similarity) or \n",
    "[search with natural language](https://docs.voxel51.com/brain.html#text-similarity) (for models that support this, such as CLIP, SigLIP, or MedSigLIP)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7ef48f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_index = fob.compute_similarity(\n",
    "    dataset,\n",
    "    model=\"google/medsiglip-448\",\n",
    "    brain_key=\"medsiglip_similarity\",\n",
    "    embeddings=\"medsiglip_embeddings\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3778b2",
   "metadata": {},
   "source": [
    "With embeddings computed, you can perform non-trivial analysis like computing scores for \n",
    "[uniqueness](https://docs.voxel51.com/brain.html#image-uniqueness), \n",
    "[representativeness](https://docs.voxel51.com/brain.html#image-representativeness), \n",
    "and [identifying near duplicates](https://docs.voxel51.com/brain.html#near-duplicates) \n",
    "with simple function calls.\n",
    "\n",
    "- Near-duplicates: Redundant images that inflate dataset size without adding value\n",
    "\n",
    "- Uniqueness: How distinct each sample is from others (low = redundant, high = informative)\n",
    "\n",
    "- Representativeness: How well a sample represents the overall distribution (high = typical, low = outlier)\n",
    "\n",
    "As an example, let's compute uniqueness.\n",
    "\n",
    "In a nutshell, uniqueness measures how far a sample is from its nearest neighbors in embedding space, with higher values indicating the sample is more isolated/distinct from other samples in the dataset.\n",
    "\n",
    "It's computed by finding each sample's K nearest neighbors (K=3), calculating a weighted average of the distances to those neighbors, and normalizing the result to a 0-1 scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "437a0b59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving embeddings from similarity index...\n",
      "Computing uniqueness...\n",
      "Uniqueness computation complete\n"
     ]
    }
   ],
   "source": [
    "# Compute uniqueness scores\n",
    "fob.compute_uniqueness(\n",
    "    dataset,\n",
    "    embeddings=\"radio_embeddings\",\n",
    "    similarity_index=sim_index\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4e5076",
   "metadata": {},
   "source": [
    "**In the App:**\n",
    "- Open the [Embeddings panel](https://docs.voxel51.com/user_guide/app.html#embeddings-panel)\n",
    "- Color by `modality` — do CT, MRI, X-ray form distinct clusters?\n",
    "- Color by `body_part` — do anatomical regions separate?\n",
    "- Color by `content_type` — do question types cluster?\n",
    "\n",
    "**What you're looking for:**\n",
    "- Clear separation = model has a chance\n",
    "- Everything mixed together = fundamental representation problem\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b8da34b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"800\"\n",
       "            src=\"http://localhost:5151/?notebook=True&subscription=7642949a-22e5-4f5e-99b8-734529a7ab1e\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x76263d82a3d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Relaunch app to see embeddings panel\n",
    "import fiftyone as fo\n",
    "session = fo.launch_app(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb110a0",
   "metadata": {},
   "source": [
    "![Explore MedGemma Embeddings](https://raw.githubusercontent.com/harpreetsahota204/medgemma_kaggle_competition/main/gifs/explore_medgemma_embeddings.gif)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30fc6cc",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Run MedGemma Inference\n",
    "\n",
    "Now let's run MedGemma 1.5 on the VQA task and store predictions.\n",
    "\n",
    "FiftyOne is open-source and hackable, with a robust framework for \n",
    "[building Plugins](https://docs.voxel51.com/plugins/developing_plugins.html) that extend \n",
    "and customize the tool. Browse this [curated collection of plugins](https://docs.voxel51.com/plugins/) \n",
    "to see integrations with various computer vision models and AI tools."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b703dd1b",
   "metadata": {},
   "source": [
    "### Register and load MedGemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "13536799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://github.com/harpreetsahota204/medgemma_1_5...\n",
      "  152.7Kb [65.3ms elapsed, ? remaining, 2.3Mb/s] \n",
      "Overwriting existing model source '/home/harpreet/fiftyone/__models__/medgemma_1_5'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "baf7a3897fe340c3a13893b95e98d648",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (incomplete total...): 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8081bbcd056947daa83344d906fd1c97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 15 files:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(<fiftyone.zoo.models.RemoteZooModel at 0x76263d894a50>,\n",
       " '/home/harpreet/fiftyone/__models__/medgemma_1_5/medgemma-1.5-4b-it')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foz.register_zoo_model_source(\n",
    "    \"https://github.com/harpreetsahota204/medgemma_1_5\",\n",
    "    overwrite=True\n",
    ")\n",
    "\n",
    "foz.download_zoo_model(\n",
    "    \"https://github.com/harpreetsahota204/medgemma_1_5\",\n",
    "    model_name=\"google/medgemma-1.5-4b-it\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "976a9b04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ab299e537894ec6a0c7b6a70dab6742",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (incomplete total...): 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2460f52a3fed41d2a47734a9c2ca86e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 15 files:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading MedGemma model from /home/harpreet/fiftyone/__models__/medgemma_1_5/medgemma-1.5-4b-it\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49ab7e82d7ce4c0e8edff0630a835669",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/883 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "medgemma = foz.load_zoo_model(\"google/medgemma-1.5-4b-it\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b87872",
   "metadata": {},
   "source": [
    "### Configure for VQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "86603789",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set operation mode\n",
    "medgemma.operation = \"classify\"\n",
    "\n",
    "# Set a custom system prompt\n",
    "medgemma.system_prompt = \"\"\"You are an expert radiologist, histopathologist, ophthalmologist, and dermatologist.\n",
    "\n",
    "Your expert opinion is needed for answering questions about medical images.\n",
    "\n",
    "Report your answer as JSON array in this format: \n",
    "\n",
    "```json\n",
    "{\n",
    "    \"classifications\": [\n",
    "        {\n",
    "            \"label\": \"descriptive medical condition or relevant label\"\n",
    "            ...,\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n",
    "Always return your response as valid JSON wrapped in ```json blocks.  You must produce only a single word answer. Do not report your confidence.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9c3158",
   "metadata": {},
   "source": [
    "### Running Inference on Multi-Question Samples\n",
    "\n",
    "Since each image has multiple Q&A pairs (`question_0`/`answer_0` through `question_19`/`answer_19`),\n",
    "we have a few options for running inference:\n",
    "\n",
    "1. **Pick one question per image** (simplest) - use `prompt_field=\"question_0\"`\n",
    "2. **Run on all questions** - loop through question fields\n",
    "3. **Flatten the dataset** - create a new sample per Q&A pair\n",
    "\n",
    "Let's start simple by running on the first question of each image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4dc8a044",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% |███████████████████| 50/50 [8.6s elapsed, 0s remaining, 5.8 samples/s]   \n"
     ]
    }
   ],
   "source": [
    "dataset.apply_model(\n",
    "    medgemma,\n",
    "    label_field=\"pred_answer_0\",\n",
    "    prompt_field=\"question_0\",  # Use the first question for each image\n",
    "    batch_size=32,\n",
    "    num_workers=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb323b9f",
   "metadata": {},
   "source": [
    "### Inspect predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "aecbb399",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Classifications: {\n",
       "    'classifications': [\n",
       "        <Classification: {\n",
       "            'id': '696ea5962b435087800e5e3b',\n",
       "            'tags': [],\n",
       "            'label': 'Chest X-ray',\n",
       "            'confidence': None,\n",
       "            'logits': None,\n",
       "        }>,\n",
       "    ],\n",
       "    'logits': None,\n",
       "}>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.first()['pred_answer_0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "695c286b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image: source_xmlab314.jpg\n",
      "Modality: X-Ray\n",
      "Q: Is this image taken via X-Ray?\n",
      "GT: Yes\n",
      "Pred: X-Ray\n",
      "--------------------------------------------------\n",
      "Image: source_xmlab42.jpg\n",
      "Modality: MRI\n",
      "Q: What modality is used to take this image?\n",
      "GT: MRI\n",
      "Pred: MRI\n",
      "--------------------------------------------------\n",
      "Image: source_xmlab361.jpg\n",
      "Modality: X-Ray\n",
      "Q: Is this image taken via X-Ray?\n",
      "GT: Yes\n",
      "Pred: X-Ray\n",
      "--------------------------------------------------\n",
      "Image: source_xmlab478.jpg\n",
      "Modality: MRI\n",
      "Q: What modality is used to take this image?\n",
      "GT: MRI\n",
      "Pred: MRI\n",
      "--------------------------------------------------\n",
      "Image: source_xmlab516.jpg\n",
      "Modality: MRI\n",
      "Q: How was this image taken?\n",
      "GT: MRI\n",
      "Pred: MRI\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Look at a few samples\n",
    "for sample in dataset.take(5):\n",
    "    print(f\"Image: {sample.filepath.split('/')[-1]}\")\n",
    "    print(f\"Modality: {sample.modality.label}\")\n",
    "    print(f\"Q: {sample.question_0}\")\n",
    "    print(f\"GT: {sample.answer_0.label if sample.answer_0 else 'None'}\")\n",
    "    print(f\"Pred: {sample.pred_answer_0.classifications[0].label if sample.pred_answer_0 else 'None'}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c43ea2",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Evaluate Performance\n",
    "\n",
    "Let's compute accuracy—but more importantly, let's slice it to find patterns.\n",
    "\n",
    "FiftyOne provides [evaluation methods](https://docs.voxel51.com/user_guide/evaluation.html) \n",
    "for various task types including [detection](https://docs.voxel51.com/user_guide/evaluation.html#detections), [classification](https://docs.voxel51.com/user_guide/evaluation.html#classifications), and [segmentation](https://docs.voxel51.com/user_guide/evaluation.html#semantic-segmentations).\n",
    "\n",
    "##### We need to make a conversion from Classifications → ⁠Classification\n",
    "\n",
    "\n",
    "The implementation of MedGemma outputs a FiftyOne *Classifications* object (notice it's plural), but to run the evaluation for classification we need a FiftyOne *Classification* (singluar)\n",
    "\n",
    "FiftyOne's `evaluate_classifications()` only works with **single-label** classification fields (`Classification`), not multilabel containers (`Classifications`).\n",
    "\n",
    "**What you need to do:**\n",
    "\n",
    "1. Choose one label per sample (e.g., first label, highest confidence)\n",
    "2. Store it as a `Classification` field\n",
    "3. Pass that field to `evaluate_classifications()`\n",
    "\n",
    "**Read more in the docs:**\n",
    "\n",
    "- [Classification evaluation overview](https://docs.voxel51.com/user_guide/evaluation.html#classifications)\n",
    "- [Simple evaluation example](https://docs.voxel51.com/user_guide/evaluation.html#id4)\n",
    "- [Binary evaluation example](https://docs.voxel51.com/user_guide/evaluation.html#binary-evaluation)\n",
    "- [Classification evaluation tutorial](https://docs.voxel51.com/tutorials/evaluate_classifications.html#Evaluating-model-with-FiftyOne)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "de69c3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "\n",
    "# assume dataset has a multilabel field \"multi\" of type fo.Classifications\n",
    "# and we want a single-label field \"single\" of type fo.Classification\n",
    "\n",
    "for sample in dataset:\n",
    "    cls_list = sample[\"pred_answer_0\"].classifications if sample[\"pred_answer_0\"] is not None else []\n",
    "\n",
    "    if cls_list:\n",
    "        # choose one classification; here we take the first\n",
    "        chosen = cls_list[0]\n",
    "        sample[\"pred_answer_0_as_cls\"] = fo.Classification(\n",
    "            label=chosen.label,\n",
    "        )\n",
    "    else:\n",
    "        sample[\"pred_answer_0_as_cls\"] = None\n",
    "\n",
    "    sample.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "09752ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the predictions in the `predictions` field with respect to the\n",
    "# labels in the `ground_truth` field\n",
    "classify_results = dataset.evaluate_classifications(\n",
    "    \"pred_answer_0_as_cls\",\n",
    "    gt_field=\"answer_0\",\n",
    "    eval_key=\"eval_ans_0\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "08b71b54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  precision    recall  f1-score   support\n",
      "\n",
      "           Axial       0.00      0.00      0.00         0\n",
      "              CT       0.75      0.38      0.50         8\n",
      "         CT scan       0.00      0.00      0.00         0\n",
      "     Chest X-ray       0.00      0.00      0.00         0\n",
      "             MRI       0.74      1.00      0.85        14\n",
      "        MRI scan       0.00      0.00      0.00         0\n",
      "              No       1.00      0.29      0.44         7\n",
      "              PA       0.00      0.00      0.00         0\n",
      "           Right       0.00      0.00      0.00         1\n",
      "Transverse Plane       0.00      0.00      0.00         2\n",
      "           X-Ray       0.20      0.12      0.15         8\n",
      "             Yes       0.00      0.00      0.00        10\n",
      "   coronal plane       0.00      0.00      0.00         0\n",
      "     lower right       0.00      0.00      0.00         0\n",
      "\n",
      "        accuracy                           0.40        50\n",
      "       macro avg       0.19      0.13      0.14        50\n",
      "    weighted avg       0.50      0.40      0.40        50\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print a classification report\n",
    "classify_results.print_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f826bd7",
   "metadata": {},
   "source": [
    "You can also open the evaluation panel in the app for a more interactive evaluation experience.\n",
    "\n",
    "You can use [Scenario Analysis](https://docs.voxel51.com/user_guide/app.html#scenario-analysis) \n",
    "for a deep dive into model behavior across different scenarios. This helps uncover edge cases, \n",
    "identify annotation errors, and understand performance variations in different contexts.\n",
    "\n",
    "![Eval MedGemma Classifications](https://raw.githubusercontent.com/harpreetsahota204/medgemma_kaggle_competition/main/gifs/medgemma_eval.gif)\n",
    "\n",
    "\n",
    "### Visual Question Answering\n",
    "\n",
    "You can also use MedGemma for visual question answering to get a more open-ended answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "da21ac1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an expert radiologist, histopathologist, ophthalmologist, and dermatologist. You are asked to provide leverage your expertise to answers to medical questions.\n",
      "\n",
      "You may be provided with a simple query, patient history with a complex query, asked to provide a medical diagnosis, or any variety of medical question.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "medgemma.operation=\"vqa\" #change operation\n",
    "\n",
    "medgemma.system_prompt = None #reset system prompt, use default system prompt for vqa\n",
    "\n",
    "print(medgemma.system_prompt) #print the default vqa system prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "94e79c99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% |███████████████████| 50/50 [24.4s elapsed, 0s remaining, 2.1 samples/s]   \n"
     ]
    }
   ],
   "source": [
    "dataset.apply_model(\n",
    "    medgemma,\n",
    "    label_field=\"free_text_answer_0\",\n",
    "    prompt_field=\"question_0\",  # Use the first question for each image\n",
    "    batch_size=32,\n",
    "    num_workers=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "27504225",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This image is a **chest X-ray (CXR)**.'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.first()['free_text_answer_0']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83638f85",
   "metadata": {},
   "source": [
    "### Running on All Questions (Optional)\n",
    "\n",
    "If you want to evaluate on all Q&A pairs, you can loop through the question fields.\n",
    "This stores predictions for each question in separate fields:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d643f5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Run inference on all questions (takes longer)\n",
    "# for i in range(20):  # Up to 20 questions per image\n",
    "#     q_field = f\"question_{i}\"\n",
    "#     pred_field = f\"free_text_answer_{i}\"\n",
    "#     \n",
    "#     # Only run if this question exists in any sample\n",
    "#     if dataset.count(q_field) > 0:\n",
    "#         print(f\"Running inference on {q_field}...\")\n",
    "#         dataset.apply_model(\n",
    "#             medgemma,\n",
    "#             label_field=pred_field,\n",
    "#             prompt_field=q_field,\n",
    "#             batch_size=32,\n",
    "#             num_workers=4,\n",
    "#         )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f6dc48",
   "metadata": {},
   "source": [
    "### Add correctness field\n",
    "\n",
    "\n",
    "Since MedGemma produces verbose answers in VQA mode, we use Gemma 3 270m as a semantic judge to determine if the predicted answer is correct rather than relying on exact string matching.\n",
    "\n",
    "Use [`values()`](https://docs.voxel51.com/api/fiftyone.core.collections.html#fiftyone.core.collections.SampleCollection.values) \n",
    "to efficiently extract field values across all samples, and \n",
    "[`set_values()`](https://docs.voxel51.com/api/fiftyone.core.collections.html#fiftyone.core.collections.SampleCollection.set_values) \n",
    "to add computed fields back to the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c01c1ac5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6272cfc1726c418daba4b68a8cd1e4ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/236 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      " 20%|██        | 10/50 [00:00<00:03, 13.27it/s]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "100%|██████████| 50/50 [00:03<00:00, 13.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50/50 answers judged as correct\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "# Load LLM judge\n",
    "judge = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=\"google/gemma-3-270m-it\",\n",
    "    device=\"cuda\",\n",
    "    dtype=\"auto\"\n",
    ")\n",
    "\n",
    "# Get data\n",
    "gt_values = dataset.values(\"free_text_answer_0\")\n",
    "pred_values = dataset.values(\"pred_answer_0\")\n",
    "questions = dataset.values(\"question_0\")\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"You are an expert medical evaluator. Your task is to determine if a predicted answer correctly answers a question, given the ground truth answer. The predicted answer may be more verbose or phrased differently, but should be semantically equivalent to the ground truth.\n",
    "\n",
    "Respond with ONLY \"CORRECT\" or \"INCORRECT\" - no other text.\"\"\"\n",
    "\n",
    "def is_correct(question, gt, pred):\n",
    "    if not pred or not gt:\n",
    "        return False\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": [{\"type\": \"text\", \"text\": SYSTEM_PROMPT}]},\n",
    "        {\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": f\"\"\"Question: {question}\n",
    "Ground Truth Answer: {gt}\n",
    "Predicted Answer: {pred}\n",
    "\n",
    "Is the Predicted Answer CORRECT or INCORRECT?\"\"\"}]}\n",
    "    ]\n",
    "    \n",
    "    output = judge(messages, max_new_tokens=16, do_sample=False)\n",
    "    return \"CORRECT\" in output[0][\"generated_text\"][-1][\"content\"].upper()\n",
    "\n",
    "# Evaluate and save\n",
    "results = [is_correct(q, gt, p) for q, gt, p in tqdm(zip(questions, gt_values, pred_values), total=len(questions))]\n",
    "dataset.set_values(\"is_correct_0\", results)\n",
    "dataset.save()\n",
    "\n",
    "print(f\"{sum(results)}/{len(results)} answers judged as correct\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "774e144e",
   "metadata": {},
   "source": [
    "### Overall accuracy (on question_0, LLM-judged)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e5470284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Accuracy (Q0, LLM-judged): 100.00% (50/50)\n"
     ]
    }
   ],
   "source": [
    "# Clean up judge pipeline to free GPU memory for subsequent operations\n",
    "from fiftyone import ViewField as F\n",
    "del judge\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Calculate overall accuracy using LLM-judged correctness\n",
    "correct = dataset.match(F(\"is_correct_0\") == True)\n",
    "total = len(dataset)\n",
    "accuracy = len(correct) / total\n",
    "\n",
    "print(f\"Overall Accuracy (Q0, LLM-judged): {accuracy:.2%} ({len(correct)}/{total})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b8a8643",
   "metadata": {},
   "source": [
    "### Accuracy by answer type\n",
    "\n",
    "CLOSED questions (yes/no) should be easier than OPEN (free-form) ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6b763893",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy by Answer Type:\n",
      "  CLOSED: 100.00% (22/22)\n",
      "  OPEN: 100.00% (28/28)\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy by Answer Type:\")\n",
    "for atype in dataset.distinct(\"answer_type.label\"):\n",
    "    view = dataset.match(F(\"answer_type.label\") == atype)\n",
    "    correct_view = view.match(F(\"is_correct_0\") == True)\n",
    "    acc = len(correct_view) / len(view) if len(view) > 0 else 0\n",
    "    print(f\"  {atype}: {acc:.2%} ({len(correct_view)}/{len(view)})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d1344d",
   "metadata": {},
   "source": [
    "### Accuracy by modality\n",
    "\n",
    "Does MedGemma perform differently on CT vs MRI vs X-Ray?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9e2218aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy by Modality:\n",
      "  CT: 100.00% (17/17)\n",
      "  MRI: 100.00% (21/21)\n",
      "  X-Ray: 100.00% (12/12)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nAccuracy by Modality:\")\n",
    "for modality in dataset.distinct(\"modality.label\"):\n",
    "    view = dataset.match(F(\"modality.label\") == modality)\n",
    "    correct_view = view.match(F(\"is_correct_0\") == True)\n",
    "    acc = len(correct_view) / len(view) if len(view) > 0 else 0\n",
    "    print(f\"  {modality}: {acc:.2%} ({len(correct_view)}/{len(view)})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c44cec9",
   "metadata": {},
   "source": [
    "### Accuracy by anatomical location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9446dcc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy by Location:\n",
      "  Abdomen: 100.00% (n=13)\n",
      "  Brain: 100.00% (n=2)\n",
      "  Brain_Tissue: 100.00% (n=15)\n",
      "  Chest_lung: 100.00% (n=3)\n",
      "  Lung: 100.00% (n=14)\n",
      "  Neck: 100.00% (n=1)\n",
      "  Pelvic Cavity: 100.00% (n=2)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nAccuracy by Location:\")\n",
    "results = []\n",
    "for location in dataset.distinct(\"location.label\"):\n",
    "    view = dataset.match(F(\"location.label\") == location)\n",
    "    correct_view = view.match(F(\"is_correct_0\") == True)\n",
    "    acc = len(correct_view) / len(view) if len(view) > 0 else 0\n",
    "    results.append((location, acc, len(view)))\n",
    "\n",
    "# Sort by accuracy\n",
    "for location, acc, n in sorted(results, key=lambda x: x[1]):\n",
    "    print(f\"  {location}: {acc:.2%} (n={n})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b15028e",
   "metadata": {},
   "source": [
    "**This is where it gets interesting.** \n",
    "\n",
    "You might find things like:\n",
    "- \"MedGemma struggles on Brain MRI images\"  \n",
    "- \"Abnormality detection is worse on Abdomen CT than Lung X-Ray\"\n",
    "- \"OPEN questions have much lower accuracy than CLOSED questions\"\n",
    "\n",
    "These are *actionable insights*, not just numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19c4059",
   "metadata": {},
   "source": [
    "## Detection with MedGemma\n",
    "\n",
    "You can use MedGemma to localize anatomical structures and pathologies in medical images. The model outputs bounding boxes in FiftyOne's Detections format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "61229050",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% |███████████████████| 50/50 [4.7m elapsed, 0s remaining, 0.2 samples/s]   \n"
     ]
    }
   ],
   "source": [
    "# Set detection mode\n",
    "medgemma.operation = \"detect\"\n",
    "\n",
    "# Get labels to detect (e.g., from ground truth)\n",
    "labels = dataset.distinct(\"detections.detections.label\")\n",
    "labels_str = \", \".join(labels)\n",
    "\n",
    "# Prompt for localization\n",
    "medgemma.prompt = f\"\"\"Locate the following in this scan: {labels_str}. \n",
    "Output the final answer in the format \"Final Answer: X\" where X is a JSON list of objects. \n",
    "The object needs a \"box_2d\" and \"label\" key. \n",
    "If the object is not present in the scan, skip it and don't output anything for that object.\n",
    "Answer:\"\"\"\n",
    "\n",
    "# Apply detection\n",
    "dataset.apply_model(\n",
    "    medgemma,\n",
    "    label_field=\"pred_detection\",\n",
    "    batch_size=32,\n",
    "    num_workers=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa937ea",
   "metadata": {},
   "source": [
    "We can then use [FiftyOne's evaluation API](https://docs.voxel51.com/user_guide/evaluation.html) to see how well the initial results. You can [`evaluate_detections()` method](https://docs.voxel51.com/user_guide/evaluation.html#detections) to evaluate the predictions of an object detection model stored in a [`Detections`](https://docs.voxel51.com/api/fiftyone.core.labels.html#fiftyone.core.labels.Detections), [`Polylines`](https://docs.voxel51.com/api/fiftyone.core.labels.html#fiftyone.core.labels.Polylines), or [`Keypoints`](https://docs.voxel51.com/api/fiftyone.core.labels.html#fiftyone.core.labels.Keypoints) field of your dataset or of a temporal detection model stored in a [`TemporalDetections`](https://docs.voxel51.com/api/fiftyone.core.labels.html#fiftyone.core.labels.TemporalDetection) field of your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f4d70489",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating detections...\n",
      " 100% |███████████████████| 50/50 [251.9ms elapsed, 0s remaining, 198.5 samples/s]      \n"
     ]
    }
   ],
   "source": [
    "results = dataset.evaluate_detections(\n",
    "    \"pred_detection\",        \n",
    "    gt_field=\"detections\",  \n",
    "    eval_key=\"initial_detection_eval\",\n",
    "    tolerance=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba306862",
   "metadata": {},
   "source": [
    "The `evaluate_detections()` method returns a [`DetectionResults` instance](https://docs.voxel51.com/api/fiftyone.utils.eval.detection.html#fiftyone.utils.eval.detection.DetectionResults) that provides a variety of methods for generating various aggregate evaluation reports about your model.\n",
    "\n",
    "In addition, when you specify an `eval_key` parameter, a number of helpful fields will be populated on each sample and its predicted/ground truth objects that you can leverage via the FiftyOne App to interactively explore the strengths and weaknesses of your model on individual samples.\n",
    "\n",
    "You can print the report to get a high-level picture of the model performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4c7d894e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           precision    recall  f1-score   support\n",
      "\n",
      "                  Bladder       0.00      0.00      0.00         1\n",
      "              Brain Edema       0.00      0.00      0.00        20\n",
      "    Brain Enhancing Tumor       0.00      0.00      0.00        10\n",
      "Brain Non-enhancing Tumor       0.00      0.00      0.00        17\n",
      "             Cardiomegaly       0.00      0.00      0.00         3\n",
      "                Clavicles       0.00      0.00      0.00         0\n",
      "                    Colon       0.00      0.00      0.00         6\n",
      "                 Duodenum       0.00      0.00      0.00         2\n",
      "                Esophagus       0.00      0.00      0.00         2\n",
      "                    Heart       0.00      0.00      0.00         3\n",
      "                 Left Eye       0.00      0.00      0.00         1\n",
      "              Left Kidney       0.00      0.00      0.00         4\n",
      "                Left Lung       0.00      0.00      0.00         4\n",
      "       Left Temporal Lobe       0.00      0.00      0.00         1\n",
      "          Lines/Catheters       0.00      0.00      0.00         0\n",
      "                    Liver       0.11      0.14      0.12        14\n",
      "                     Mass       0.00      0.00      0.00         3\n",
      "                   Nodule       0.00      0.00      0.00         3\n",
      "                Pneumonia       0.00      0.00      0.00         3\n",
      "                   Rectum       0.00      0.00      0.00         1\n",
      "                     Ribs       0.00      0.00      0.00         0\n",
      "                Right Eye       0.00      0.00      0.00         1\n",
      "             Right Kidney       0.00      0.00      0.00         3\n",
      "               Right Lung       0.00      0.00      0.00         5\n",
      "      Right Temporal Lobe       0.00      0.00      0.00         1\n",
      "                 Scapulae       0.00      0.00      0.00         0\n",
      "              Small Bowel       0.00      0.00      0.00         9\n",
      "              Spinal Cord       0.00      0.00      0.00         9\n",
      "                   Spleen       0.00      0.00      0.00         7\n",
      "                  Stomach       0.00      0.00      0.00         1\n",
      "                  Trachea       0.00      0.00      0.00         1\n",
      "                Vertebrae       0.00      0.00      0.00         0\n",
      "\n",
      "                micro avg       0.01      0.01      0.01       135\n",
      "                macro avg       0.00      0.00      0.00       135\n",
      "             weighted avg       0.01      0.01      0.01       135\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results.print_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddda8c24",
   "metadata": {},
   "source": [
    "You can inspect the quality of the detections also use the model evaluation panel in the app:\n",
    "\n",
    "\n",
    "![Eval MedGemma Classifications](https://raw.githubusercontent.com/harpreetsahota204/medgemma_kaggle_competition/main/gifs/medgemma_eval_detections.gif)\n",
    "\n",
    "\n",
    "\n",
    "The results look...not great.\n",
    "\n",
    "But, this means we have a starting point. Now that we know the model can predict bounding boxes we can fine-tune it on our dataset!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724416ca",
   "metadata": {},
   "source": [
    "If you're running this notebook end to end, then it's a good idea to clear up some GPU memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d271f7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "del medgemma\n",
    "del medsiglip\n",
    "\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc5bd88",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Fine-Tuning MedGemma for Localization\n",
    "\n",
    "You've explored the data, identified failure patterns, and have hypotheses about what to fix.\n",
    "Now let's fine-tune MedGemma to output bounding box coordinates for localization tasks.\n",
    "\n",
    "This section demonstrates converting datasets to PyTorch format for training.\n",
    "\n",
    "We'll follow these steps:\n",
    "1. Define a [`GetItem`](https://docs.voxel51.com/api/fiftyone.utils.torch.html#fiftyone.utils.torch.GetItem) subclass to extract and transform data from FiftyOne\n",
    "2. Create train/val splits and flatten detections using [`to_patches()`](https://docs.voxel51.com/api/fiftyone.core.collections.html#fiftyone.core.collections.SampleCollection.to_patches)\n",
    "3. Convert to PyTorch datasets using [`to_torch()`](https://docs.voxel51.com/api/fiftyone.core.collections.html#fiftyone.core.collections.SampleCollection.to_torch)\n",
    "4. Set up QLoRA fine-tuning with the TRL library's `SFTTrainer`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f74ac2",
   "metadata": {},
   "source": [
    "### Install fine-tuning dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "29e1e592",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade --quiet bitsandbytes peft trl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec2e337",
   "metadata": {},
   "source": [
    "### Step 1: Define the GetItem subclass\n",
    "\n",
    "FiftyOne's [`GetItem`](https://docs.voxel51.com/api/fiftyone.utils.torch.html#fiftyone.utils.torch.GetItem) \n",
    "class is the bridge between FiftyOne and PyTorch. It tells FiftyOne:\n",
    "\n",
    "1. **What fields to extract** from each sample (via `required_keys`)\n",
    "2. **How to transform them** into your desired format (via `__call__`)\n",
    "\n",
    "The `field_mapping` parameter is important when working with patches. In a patches view,\n",
    "the detection data lives in the original field name (e.g., \"detections\"), but we want \n",
    "to access it with a generic name in our code.\n",
    "\n",
    "`field_mapping={\"detection\": \"detections\"}` means:\n",
    "- In our code, we write `d[\"detection\"]`\n",
    "- FiftyOne knows to pull from the \"detections\" field\n",
    "\n",
    "This makes our `GetItem` reusable across datasets with different field names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b8eb7a1",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "from PIL import Image\n",
    "from fiftyone.utils.torch import GetItem\n",
    "\n",
    "# System prompt for localization task\n",
    "LOCALIZATION_SYSTEM_PROMPT = \"\"\"Instructions:\n",
    "The following user query will require outputting bounding boxes. The format of bounding boxes coordinates is [y0, x0, y1, x1] where (y0, x0) must be top-left corner and (y1, x1) the bottom-right corner. This implies that x0 < x1 and y0 < y1. Always normalize the x and y coordinates the range [0, 1000], meaning that a bounding box starting at 15% of the image width would be associated with an x coordinate of 150. You MUST output a single parseable json list of objects enclosed into ```json...``` brackets, for instance ```json[{\"box_2d\": [800, 3, 840, 471], \"label\": \"car\"}, {\"box_2d\": [400, 22, 600, 73], \"label\": \"dog\"}]``` is a valid output. Now answer to the user query.\n",
    "\n",
    "Remember \"left\" refers to the patient's left side where the heart is and sometimes underneath an L in the upper right corner of the image.\"\"\"\n",
    "\n",
    "\n",
    "class LocalizationGetItem(GetItem):\n",
    "    \"\"\"\n",
    "    Extracts and transforms detection data for MedGemma localization fine-tuning.\n",
    "    \n",
    "    Each patch sample (after to_patches()) contains:\n",
    "    - filepath: path to the full image\n",
    "    - detection: the Detection object (bbox, label, etc.)\n",
    "    - metadata: image dimensions\n",
    "    \n",
    "    We transform this into MedGemma's expected message format with:\n",
    "    - System prompt explaining the bbox output format\n",
    "    - User message with the localization query\n",
    "    - Assistant message with the target bbox in JSON format\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, field_mapping=None):\n",
    "        # Must call super().__init__() with field_mapping - this sets up\n",
    "        # the internal mapping that FiftyOne uses to pull the right fields\n",
    "        super().__init__(field_mapping=field_mapping)\n",
    "    \n",
    "    @property\n",
    "    def required_keys(self):\n",
    "        # These are the keys we'll access in __call__.\n",
    "        # 'detection' is a virtual key that gets mapped to the real field\n",
    "        # via field_mapping. 'filepath' and 'metadata' are standard fields\n",
    "        # that exist on all FiftyOne samples.\n",
    "        return [\"filepath\", \"detection\", \"metadata\"]\n",
    "    \n",
    "    def __call__(self, d):\n",
    "        \"\"\"\n",
    "        Transform a FiftyOne sample dict into MedGemma fine-tuning format.\n",
    "        \n",
    "        This is where the FiftyOne → MedGemma conversion happens:\n",
    "        - FiftyOne bbox format [x, y, w, h] in [0,1] \n",
    "        - MedGemma format [y0, x0, y1, x1] normalized to [0, 1000]\n",
    "        \"\"\"\n",
    "        filepath = d[\"filepath\"]\n",
    "        detection = d[\"detection\"]\n",
    "        \n",
    "        # Get the label from the detection\n",
    "        label = detection.label\n",
    "        \n",
    "        # --- Bounding Box Conversion ---\n",
    "        # FiftyOne stores bboxes as [x, y, width, height] with values in [0, 1]\n",
    "        # MedGemma expects [y0, x0, y1, x1] normalized to [0, 1000]\n",
    "        rx, ry, rw, rh = detection.bounding_box\n",
    "        \n",
    "        # Convert to [y0, x0, y1, x1] format, scaled to [0, 1000]\n",
    "        x0 = int(rx * 1000)\n",
    "        y0 = int(ry * 1000)\n",
    "        x1 = int((rx + rw) * 1000)\n",
    "        y1 = int((ry + rh) * 1000)\n",
    "        \n",
    "        # Format as [y0, x0, y1, x1] per the prompt instructions\n",
    "        bbox_normalized = [y0, x0, y1, x1]\n",
    "        \n",
    "        # --- Construct Messages ---\n",
    "        # Format the target response as JSON\n",
    "        target_json = f'```json[{{\"box_2d\": {bbox_normalized}, \"label\": \"{label}\"}}]```'\n",
    "        \n",
    "        # Build the message payload in chat format\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"image\"},\n",
    "                    {\"type\": \"text\", \"text\": f\"{LOCALIZATION_SYSTEM_PROMPT}\\n\\nLocate the {label} in this medical image.\"},\n",
    "                ],\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": target_json},\n",
    "                ],\n",
    "            },\n",
    "        ]\n",
    "        \n",
    "        return {\n",
    "            \"filepath\": filepath,\n",
    "            \"image\": Image.open(filepath).convert(\"RGB\"),\n",
    "            \"messages\": messages,\n",
    "            \"label\": label,\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e60a36d",
   "metadata": {},
   "source": [
    "### Step 2: Create train/val split and flatten detections\n",
    "\n",
    "Since our dataset doesn't have existing train/val [tags](https://docs.voxel51.com/user_guide/basics.html#tags), \n",
    "we'll create them using [`random_split()`](https://docs.voxel51.com/api/fiftyone.utils.random.html#fiftyone.utils.random.random_split).\n",
    "\n",
    "Then we use [`to_patches()`](https://docs.voxel51.com/api/fiftyone.core.collections.html#fiftyone.core.collections.SampleCollection.to_patches) \n",
    "to flatten the dataset so each detection becomes its own sample.\n",
    "\n",
    "**Key insight:** `to_patches(field)` creates a view where each detection in that field becomes \n",
    "its own sample. If you have 100 images with 5 detections each, `to_patches` gives you 500 patch samples. \n",
    "This is perfect for instance-level training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "376188c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples with detections: 50\n"
     ]
    }
   ],
   "source": [
    "import fiftyone.utils.random as four\n",
    "from fiftyone import ViewField as F\n",
    "\n",
    "# Filter to samples that have detections\n",
    "has_detections_view = dataset.match(F(\"detections\") != None)\n",
    "print(f\"Samples with detections: {len(has_detections_view)}\")\n",
    "\n",
    "# Create train/val split (80/20)\n",
    "four.random_split(has_detections_view, {\"train\": 0.8, \"val\": 0.2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20e62fad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples - train: 40, val: 10\n"
     ]
    }
   ],
   "source": [
    "# Filter by split tags using match_tags()\n",
    "# https://docs.voxel51.com/api/fiftyone.core.collections.html#fiftyone.core.collections.SampleCollection.match_tags\n",
    "train_view = has_detections_view.match_tags(\"train\")\n",
    "val_view = has_detections_view.match_tags(\"val\")\n",
    "\n",
    "print(f\"Samples - train: {len(train_view)}, val: {len(val_view)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "758b73b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patches - train: 115, val: 20\n"
     ]
    }
   ],
   "source": [
    "# Flatten using to_patches() - each detection becomes its own sample\n",
    "train_patches = train_view.to_patches(\"detections\")\n",
    "val_patches = val_view.to_patches(\"detections\")\n",
    "\n",
    "print(f\"Patches - train: {len(train_patches)}, val: {len(val_patches)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8499de29",
   "metadata": {},
   "source": [
    "### Step 3: Convert to PyTorch datasets\n",
    "\n",
    "Use [`to_torch()`](https://docs.voxel51.com/api/fiftyone.core.collections.html#fiftyone.core.collections.SampleCollection.to_torch) \n",
    "with our `GetItem` class to create PyTorch-compatible datasets.\n",
    "\n",
    "In the patches view, each sample's detection data lives in the original field (e.g., \"detections\"). \n",
    "The `field_mapping` lets us access it with a generic name in our `GetItem` code, making the class \n",
    "reusable across different datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cddcc44d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 115\n",
      "Val dataset size: 20\n"
     ]
    }
   ],
   "source": [
    "# Set up field mapping - in patches view, each sample's detection data \n",
    "# lives in the original field \"detections\"\n",
    "field_mapping = {\"detection\": \"detections\"}\n",
    "\n",
    "# Create GetItem instances\n",
    "train_getter = LocalizationGetItem(field_mapping=field_mapping)\n",
    "val_getter = LocalizationGetItem(field_mapping=field_mapping)\n",
    "\n",
    "# Convert to PyTorch datasets\n",
    "train_dataset = train_patches.to_torch(train_getter)\n",
    "val_dataset = val_patches.to_torch(val_getter)\n",
    "\n",
    "print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "print(f\"Val dataset size: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "959634c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample keys: dict_keys(['filepath', 'image', 'messages', 'label'])\n",
      "Label: Cardiomegaly\n",
      "Messages structure:\n",
      "  Role: user\n",
      "  Role: assistant\n"
     ]
    }
   ],
   "source": [
    "# Verify the data format\n",
    "sample = train_dataset[0]\n",
    "print(\"Sample keys:\", sample.keys())\n",
    "print(\"Label:\", sample[\"label\"])\n",
    "print(\"Messages structure:\")\n",
    "for msg in sample[\"messages\"]:\n",
    "    print(f\"  Role: {msg['role']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4286f6b",
   "metadata": {},
   "source": [
    "### Step 4: Load MedGemma with QLoRA configuration\n",
    "\n",
    "We use 4-bit quantization (QLoRA) to reduce memory requirements while maintaining\n",
    "fine-tuning capability. This allows fine-tuning on consumer GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d4553265",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "543b6429462b408ab874392aea1994ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/883 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "from transformers import AutoProcessor, AutoModelForImageTextToText, BitsAndBytesConfig\n",
    "\n",
    "model_id = \"google/medgemma-1.5-4b-it\"\n",
    "\n",
    "model_kwargs = dict(\n",
    "    attn_implementation=\"eager\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map={\"\": 0},\n",
    "    quantization_config=BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        bnb_4bit_quant_storage=torch.bfloat16,\n",
    "    ),\n",
    ")\n",
    "\n",
    "model = AutoModelForImageTextToText.from_pretrained(model_id, **model_kwargs)\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "processor.tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85093c9d",
   "metadata": {},
   "source": [
    "### Step 5: Configure LoRA\n",
    "\n",
    "LoRA (Low-Rank Adaptation) allows efficient fine-tuning by only training \n",
    "small adapter matrices instead of all model weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bb60f41a",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "from peft import LoraConfig\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    r=16,\n",
    "    bias=\"none\",\n",
    "    target_modules=\"all-linear\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    modules_to_save=[\n",
    "        \"lm_head\",\n",
    "        \"embed_tokens\",\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09aae91b",
   "metadata": {},
   "source": [
    "### Step 6: Define the collate function\n",
    "\n",
    "The collate function processes batches by:\n",
    "1. Applying the chat template to format messages\n",
    "2. Processing images and text together\n",
    "3. Creating labels with proper masking for padding and image tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "daf8d8aa",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def collate_fn(examples: list[dict[str, Any]]):\n",
    "    texts = []\n",
    "    images = []\n",
    "    \n",
    "    for example in examples:\n",
    "        # Convert image to RGB and wrap in list (processor expects list of images per sample)\n",
    "        images.append([example[\"image\"].convert(\"RGB\")])\n",
    "        \n",
    "        # Apply chat template to format the conversation\n",
    "        texts.append(processor.apply_chat_template(\n",
    "            example[\"messages\"], \n",
    "            add_generation_prompt=False, \n",
    "            tokenize=False\n",
    "        ).strip())\n",
    "    \n",
    "    # Tokenize texts and process images\n",
    "    batch = processor(text=texts, images=images, return_tensors=\"pt\", padding=True)\n",
    "    \n",
    "    # Create labels from input_ids\n",
    "    # We mask padding tokens and image tokens so they don't contribute to loss\n",
    "    labels = batch[\"input_ids\"].clone()\n",
    "    \n",
    "    # Get the image token ID to mask it\n",
    "    image_token_id = processor.tokenizer.convert_tokens_to_ids(\n",
    "        processor.tokenizer.special_tokens_map[\"boi_token\"]\n",
    "    )\n",
    "    \n",
    "    # Mask tokens that should not be used in loss computation\n",
    "    labels[labels == processor.tokenizer.pad_token_id] = -100\n",
    "    labels[labels == image_token_id] = -100\n",
    "    labels[labels == 262144] = -100  # Additional image token ID\n",
    "    \n",
    "    batch[\"labels\"] = labels\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d684dc",
   "metadata": {},
   "source": [
    "### Step 7: Configure training\n",
    "\n",
    "We use TRL's `SFTConfig` and `SFTTrainer` for a clean training setup with\n",
    "all the best practices built in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9bd6ede8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTConfig, SFTTrainer\n",
    "\n",
    "num_train_epochs = 1  # Adjust based on your needs\n",
    "learning_rate = 2e-4\n",
    "\n",
    "training_args = SFTConfig(\n",
    "    output_dir=\"medgemma-localization-lora\",         # Directory to save the model\n",
    "    num_train_epochs=num_train_epochs,               # Number of training epochs\n",
    "    per_device_train_batch_size=4,                   # Batch size per device during training\n",
    "    per_device_eval_batch_size=4,                    # Batch size per device during evaluation\n",
    "    gradient_accumulation_steps=4,                   # Number of steps before performing a backward/update pass\n",
    "    gradient_checkpointing=True,                     # Enable gradient checkpointing to reduce memory usage\n",
    "    optim=\"adamw_torch_fused\",                       # Use fused AdamW optimizer for better performance\n",
    "    logging_steps=50,                                # Number of steps between logs\n",
    "    save_strategy=\"epoch\",                           # Save checkpoint every epoch\n",
    "    eval_strategy=\"steps\",                           # Evaluate every `eval_steps`\n",
    "    eval_steps=50,                                   # Number of steps between evaluations\n",
    "    learning_rate=learning_rate,                     # Learning rate\n",
    "    bf16=True,                                       # Use bfloat16 precision\n",
    "    max_grad_norm=0.3,                               # Max gradient norm\n",
    "    warmup_steps=5,                               # Warmup steps\n",
    "    lr_scheduler_type=\"linear\",                      # Use linear learning rate scheduler\n",
    "    push_to_hub=False,                               # Set to True to push model to Hub\n",
    "    report_to=\"tensorboard\",                         # Report metrics to tensorboard\n",
    "    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
    "    dataset_kwargs={\"skip_prepare_dataset\": True},   # We preprocess manually\n",
    "    remove_unused_columns=False,                     # Keep columns for data collator\n",
    "    label_names=[\"labels\"],                          # Input keys that correspond to labels\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14dc20d7",
   "metadata": {},
   "source": [
    "### Step 8: Create trainer and train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5838650",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Workaround for MedGemma 1.5's SiglipVisionTransformer\n",
    "from transformers.models.siglip.modeling_siglip import SiglipVisionTransformer\n",
    "SiglipVisionTransformer.get_input_embeddings = lambda self: None\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    peft_config=peft_config,\n",
    "    processing_class=processor,\n",
    "    data_collator=collate_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e94c12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 1, 'bos_token_id': 2, 'pad_token_id': 0}.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='8' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2/8 : < :, Epoch 0.14/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20526cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the fine-tuned model\n",
    "trainer.save_model()\n",
    "\n",
    "# Optional: Push to Hugging Face Hub\n",
    "# trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ea4646",
   "metadata": {},
   "source": [
    "### Clean up GPU memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13634f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "del trainer\n",
    "\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068b7a4f",
   "metadata": {},
   "source": [
    "## Evaluating Your Fine-Tuned Model\n",
    "\n",
    "Of course, the above is just a blueprint for what to do. For the best results, you need to figure out the right data to train on as well as the training recipe.\n",
    "\n",
    "Once you've fine-tuned MedGemma for localization, go back through the earlier \n",
    "sections of this notebook to evaluate how well your model performs:\n",
    "\n",
    "1. **Load your fine-tuned model** and run inference on the validation set. To do this, you will need to [fork my implementation of MedGemma 1.5](https://github.com/harpreetsahota204/medgemma_1_5) as a remote zoo model and update the [model maifest](https://github.com/harpreetsahota204/medgemma_1_5/blob/main/manifest.json) to download your weights. You may also need to make changes to the [zoo.py](https://github.com/harpreetsahota204/medgemma_1_5/blob/main/zoo.py) to merge your LORA with the original model. This is an exercise left to you.\n",
    "2. **Store predictions** in FiftyOne alongside the ground truth\n",
    "3. **Use the evaluation techniques** from Sections 5 and 6:\n",
    "   - Compute accuracy by modality, body part, and content type\n",
    "   - Analyze errors using the App and similarity search\n",
    "   - Tag patterns in failures\n",
    "\n",
    "You can use FiftyOne's [`evaluate_detections()` method](https://docs.voxel51.com/user_guide/evaluation.html#detections) \n",
    "to evaluate object detection predictions, computing metrics like mAP and per-class performance.\n",
    "\n",
    "This iterative workflow—explore, model, evaluate, fine-tune—is how you systematically\n",
    "improve your model's performance on specific failure modes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c6b0ad",
   "metadata": {},
   "source": [
    "---\n",
    "## Bringing It All Together\n",
    "\n",
    "Here's what you've learned to do:\n",
    "\n",
    "| Step | What You Did | Why It Matters |\n",
    "|------|-------------|----------------|\n",
    "| Load & Explore | Understood data distribution before modeling | Caught potential issues early |\n",
    "| Embeddings | Visualized MedSigLIP clusters | Diagnosed whether classes are separable |\n",
    "| Inference | Ran MedGemma, stored predictions with data | Everything in one place for analysis |\n",
    "| Evaluation | Sliced accuracy by modality, location, etc. | Found *where* the model fails |\n",
    "| Error Analysis | Visualized failures, tagged patterns | Understood *why* it fails |\n",
    "| Fine-Tuning | Used GetItem + SFTTrainer for localization | Improved model on specific failure modes |\n",
    "\n",
    "**The workflow you built here works for any dataset, any model, any challenge.**\n",
    "\n",
    "Whether you're doing:\n",
    "- Chest X-ray report generation\n",
    "- Dermatology classification  \n",
    "- CT severity assessment\n",
    "- Histopathology analysis\n",
    "\n",
    "The pattern is the same:\n",
    "1. Organize your data in FiftyOne\n",
    "2. Understand it before modeling\n",
    "3. Run inference, store predictions\n",
    "4. Slice, visualize, debug\n",
    "5. Fine-tune and iterate\n",
    "\n",
    "**Now go win that challenge.** 🏆"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5460be53",
   "metadata": {},
   "source": [
    "---\n",
    "## Resources\n",
    "\n",
    "### FiftyOne Documentation\n",
    "- [FiftyOne Documentation](https://docs.voxel51.com/)\n",
    "- [FiftyOne Datasets](https://docs.voxel51.com/user_guide/using_datasets.html)\n",
    "- [FiftyOne Views Cheat Sheet](https://docs.voxel51.com/cheat_sheets/views_cheat_sheet.html)\n",
    "- [FiftyOne Filtering Cheat Sheet](https://docs.voxel51.com/cheat_sheets/filtering_cheat_sheet.html)\n",
    "- [FiftyOne PyTorch Integration](https://docs.voxel51.com/integrations/pytorch.html)\n",
    "- [FiftyOne Brain](https://docs.voxel51.com/brain.html) (embeddings, similarity, visualization)\n",
    "- [FiftyOne Evaluation](https://docs.voxel51.com/user_guide/evaluation.html)\n",
    "- [FiftyOne Model Zoo](https://docs.voxel51.com/model_zoo/overview.html)\n",
    "- [FiftyOne Plugins](https://docs.voxel51.com/plugins/)\n",
    "\n",
    "### Dataset & Models\n",
    "- [SLAKE Dataset on HuggingFace](https://huggingface.co/datasets/Voxel51/SLAKE)\n",
    "- [MedGemma Model Card](https://huggingface.co/google/medgemma-1.5-4b-it)\n",
    "- [MedSigLIP Model Card](https://huggingface.co/google/medsiglip-448)\n",
    "\n",
    "### Fine-Tuning\n",
    "- [TRL SFTTrainer Documentation](https://huggingface.co/docs/trl/sft_trainer)\n",
    "- [PEFT LoRA Documentation](https://huggingface.co/docs/peft/main/en/conceptual_guides/lora)\n",
    "\n",
    "### Competition\n",
    "- [MedGemma Impact Challenge](https://www.kaggle.com/competitions/med-gemma-impact-challenge)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
