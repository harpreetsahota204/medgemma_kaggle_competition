{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/harpreetsahota204/medgemma_kaggle_competition/blob/main/medgemma_impact_starter.ipynb)\n",
    "\n",
    "# Winning the MedGemma Impact Challenge with FiftyOne\n",
    "\n",
    "**The difference between a demo and a winning submission is understanding where your model breaksâ€”and why.**\n",
    "\n",
    "This notebook shows you how to use FiftyOne as your workbench for the MedGemma Impact Challenge. \n",
    "We'll go beyond running inference and printing metrics. You'll learn to:\n",
    "\n",
    "1. **Explore your data** before modeling\n",
    "2. **Visualize embeddings** to diagnose learnability  \n",
    "3. **Run MedGemma inference** and store predictions alongside ground truth\n",
    "4. **Analyze failures** systematicallyâ€”not just count them\n",
    "5. **Export curated subsets** for fine-tuning\n",
    "\n",
    "We'll use the [SLAKE dataset](https://huggingface.co/datasets/Voxel51/SLAKE), a medical VQA benchmark \n",
    "with images from multiple modalities (CT, MRI, X-ray), rich annotations including bounding boxes and \n",
    "segmentation masks, and questions spanning anatomy, abnormalities, and more.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup & Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U fiftyone huggingface_hub accelerate sentencepiece protobuf torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "import fiftyone.zoo as foz\n",
    "import fiftyone.brain as fob\n",
    "from fiftyone import ViewField as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Authenticate with Hugging Face\n",
    "\n",
    "Both MedGemma and MedSigLIP are gated models. You'll need to:\n",
    "1. Request access on [MedGemma](https://huggingface.co/google/medgemma-1.5-4b-it) and [MedSigLIP](https://huggingface.co/google/medsiglip-448)\n",
    "2. Set your HF token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# os.environ[\"HF_TOKEN\"] = \"your_token_here\"\n",
    "\n",
    "# Or login via CLI: huggingface-cli login"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Load the SLAKE Dataset\n",
    "\n",
    "The SLAKE dataset is already in FiftyOne format on Hugging Face. One line to load it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fiftyone.utils.huggingface import load_from_hub\n",
    "\n",
    "dataset = load_from_hub(\n",
    "    \"Voxel51/SLAKE\",\n",
    "    name=\"SLAKE\",\n",
    "    overwrite=True,\n",
    ")\n",
    "\n",
    "print(f\"Loaded {len(dataset)} samples\")\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the schema to understand what we're working with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.print_field_schema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each sample has:\n",
    "- **question**: The VQA question\n",
    "- **answer**: Ground truth answer\n",
    "- **question_type**: \"open\" or \"closed\" (free-form vs multiple choice)\n",
    "- **content_type**: What the question is about (e.g., \"Organ\", \"Abnormality\", \"Position\")\n",
    "- **modality**: Imaging modality (CT, MRI, X-Ray)\n",
    "- **body_part**: Anatomical region\n",
    "- **mask**: Segmentation mask (where available)\n",
    "- **detections**: Bounding box annotations with labels (where available)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Explore Your Data (Before You Model)\n",
    "\n",
    "Don't rush to inference. Understanding your data distribution is how you catch problems early."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What modalities do we have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Modalities:\")\n",
    "for modality, count in dataset.count_values(\"modality\").items():\n",
    "    print(f\"  {modality}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What types of questions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nQuestion Types:\")\n",
    "for qtype, count in dataset.count_values(\"question_type\").items():\n",
    "    print(f\"  {qtype}: {count}\")\n",
    "\n",
    "print(\"\\nContent Types:\")\n",
    "for ctype, count in sorted(dataset.count_values(\"content_type\").items(), key=lambda x: -x[1]):\n",
    "    print(f\"  {ctype}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What body parts are covered?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nBody Parts:\")\n",
    "for part, count in sorted(dataset.count_values(\"body_part\").items(), key=lambda x: -x[1]):\n",
    "    print(f\"  {part}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Launch the App to explore visually\n",
    "\n",
    "This is where FiftyOne shines. Filter, sort, and browse your data interactively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = fo.launch_app(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Try these in the App:**\n",
    "- Filter to `modality == \"CT\"` \n",
    "- Filter to `content_type == \"Abnormality\"`\n",
    "- Look at samples with bounding boxes vs without\n",
    "\n",
    "You'll start to notice patterns: certain body parts have more questions, \n",
    "certain modalities are over/under-represented, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create useful views\n",
    "\n",
    "Save views for subsets you'll come back to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Closed questions only (easier to evaluate)\n",
    "closed_questions = dataset.match(F(\"question_type\") == \"closed\")\n",
    "print(f\"Closed questions: {len(closed_questions)}\")\n",
    "\n",
    "# Questions with localization annotations\n",
    "has_detections = dataset.match(F(\"detections\") != None)\n",
    "print(f\"Samples with detections: {len(has_detections)}\")\n",
    "\n",
    "# CT images only\n",
    "ct_only = dataset.match(F(\"modality\") == \"CT\")\n",
    "print(f\"CT images: {len(ct_only)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Compute Embeddings with MedSigLIP\n",
    "\n",
    "Before running VQA inference, let's see if the embedding space even separates our classes.\n",
    "If MedSigLIP embeddings don't cluster by modality or body part, that's diagnostic information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Register and load MedSigLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the model source (one time)\n",
    "foz.register_zoo_model_source(\n",
    "    \"https://github.com/harpreetsahota204/medsiglip\",\n",
    "    overwrite=True\n",
    ")\n",
    "\n",
    "# Download the model (one time)\n",
    "foz.download_zoo_model(\n",
    "    \"https://github.com/harpreetsahota204/medsiglip\",\n",
    "    model_name=\"google/medsiglip-448\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "medsiglip = foz.load_zoo_model(\"google/medsiglip-448\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.compute_embeddings(\n",
    "    model=medsiglip,\n",
    "    embeddings_field=\"medsiglip_embeddings\",\n",
    ")\n",
    "\n",
    "print(\"Embeddings computed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize in 2D\n",
    "\n",
    "Project to 2D with UMAP and see if natural clusters emerge:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = fob.compute_visualization(\n",
    "    dataset,\n",
    "    embeddings=\"medsiglip_embeddings\",\n",
    "    method=\"umap\",\n",
    "    brain_key=\"medsiglip_viz\",\n",
    "    num_dims=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relaunch app to see embeddings panel\n",
    "session = fo.launch_app(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In the App:**\n",
    "- Open the Embeddings panel\n",
    "- Color by `modality` â€” do CT, MRI, X-ray form distinct clusters?\n",
    "- Color by `body_part` â€” do anatomical regions separate?\n",
    "- Color by `content_type` â€” do question types cluster?\n",
    "\n",
    "**What you're looking for:**\n",
    "- Clear separation = model has a chance\n",
    "- Everything mixed together = fundamental representation problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a similarity index for later\n",
    "\n",
    "This lets us find similar images, which is useful for error analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_index = fob.compute_similarity(\n",
    "    dataset,\n",
    "    model=\"google/medsiglip-448\",\n",
    "    brain_key=\"medsiglip_similarity\",\n",
    "    embeddings=\"medsiglip_embeddings\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Run MedGemma Inference\n",
    "\n",
    "Now let's run MedGemma 1.5 on the VQA task and store predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Register and load MedGemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foz.register_zoo_model_source(\n",
    "    \"https://github.com/harpreetsahota204/medgemma_1_5\",\n",
    "    overwrite=True\n",
    ")\n",
    "\n",
    "foz.download_zoo_model(\n",
    "    \"https://github.com/harpreetsahota204/medgemma_1_5\",\n",
    "    model_name=\"google/medgemma-1.5-4b-it\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "medgemma = foz.load_zoo_model(\"google/medgemma-1.5-4b-it\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure for VQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set operation mode\n",
    "medgemma.operation = \"vqa\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run inference using per-sample prompts\n",
    "\n",
    "Each sample has its own question stored in the `question` field. \n",
    "We use `prompt_field` to read prompts from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run on full dataset (or a subset for testing)\n",
    "# For faster iteration, start with a subset:\n",
    "# subset = dataset.take(100)\n",
    "\n",
    "dataset.apply_model(\n",
    "    medgemma,\n",
    "    label_field=\"medgemma_answer\",\n",
    "    prompt_field=\"question\",  # Each sample uses its own question\n",
    "    batch_size=32,\n",
    "    num_workers=4,\n",
    ")\n",
    "\n",
    "print(\"Inference complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at a few samples\n",
    "for sample in dataset.take(5):\n",
    "    print(f\"Q: {sample.question}\")\n",
    "    print(f\"GT: {sample.answer}\")\n",
    "    print(f\"Pred: {sample.medgemma_answer.label if sample.medgemma_answer else 'None'}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Evaluate Performance\n",
    "\n",
    "Let's compute accuracyâ€”but more importantly, let's slice it to find patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add correctness field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For closed questions, we can do exact match\n",
    "# For open questions, you'd want semantic similarity\n",
    "\n",
    "def normalize_answer(ans):\n",
    "    \"\"\"Basic normalization for answer comparison\"\"\"\n",
    "    if ans is None:\n",
    "        return \"\"\n",
    "    return str(ans).lower().strip()\n",
    "\n",
    "# Add is_correct field\n",
    "for sample in dataset.iter_samples(autosave=True):\n",
    "    gt = normalize_answer(sample.answer)\n",
    "    pred = normalize_answer(\n",
    "        sample.medgemma_answer.label if sample.medgemma_answer else None\n",
    "    )\n",
    "    sample[\"is_correct\"] = (gt == pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overall accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = dataset.match(F(\"is_correct\") == True)\n",
    "total = len(dataset)\n",
    "accuracy = len(correct) / total\n",
    "\n",
    "print(f\"Overall Accuracy: {accuracy:.2%} ({len(correct)}/{total})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy by question type\n",
    "\n",
    "Closed questions should be easier than open-ended ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for qtype in dataset.distinct(\"question_type\"):\n",
    "    view = dataset.match(F(\"question_type\") == qtype)\n",
    "    correct_view = view.match(F(\"is_correct\") == True)\n",
    "    acc = len(correct_view) / len(view) if len(view) > 0 else 0\n",
    "    print(f\"{qtype}: {acc:.2%} ({len(correct_view)}/{len(view)})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy by modality\n",
    "\n",
    "Does MedGemma perform differently on CT vs MRI vs X-ray?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nAccuracy by Modality:\")\n",
    "for modality in dataset.distinct(\"modality\"):\n",
    "    view = dataset.match(F(\"modality\") == modality)\n",
    "    correct_view = view.match(F(\"is_correct\") == True)\n",
    "    acc = len(correct_view) / len(view) if len(view) > 0 else 0\n",
    "    print(f\"  {modality}: {acc:.2%} ({len(correct_view)}/{len(view)})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy by body part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nAccuracy by Body Part:\")\n",
    "results = []\n",
    "for part in dataset.distinct(\"body_part\"):\n",
    "    view = dataset.match(F(\"body_part\") == part)\n",
    "    correct_view = view.match(F(\"is_correct\") == True)\n",
    "    acc = len(correct_view) / len(view) if len(view) > 0 else 0\n",
    "    results.append((part, acc, len(view)))\n",
    "\n",
    "# Sort by accuracy\n",
    "for part, acc, n in sorted(results, key=lambda x: x[1]):\n",
    "    print(f\"  {part}: {acc:.2%} (n={n})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy by content type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nAccuracy by Content Type:\")\n",
    "results = []\n",
    "for ctype in dataset.distinct(\"content_type\"):\n",
    "    view = dataset.match(F(\"content_type\") == ctype)\n",
    "    correct_view = view.match(F(\"is_correct\") == True)\n",
    "    acc = len(correct_view) / len(view) if len(view) > 0 else 0\n",
    "    results.append((ctype, acc, len(view)))\n",
    "\n",
    "for ctype, acc, n in sorted(results, key=lambda x: x[1]):\n",
    "    print(f\"  {ctype}: {acc:.2%} (n={n})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This is where it gets interesting.** \n",
    "\n",
    "You might find things like:\n",
    "- \"MedGemma struggles on Position questions for Abdomen CT\"  \n",
    "- \"Abnormality detection is worse on MRI than X-ray\"\n",
    "\n",
    "These are *actionable insights*, not just numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Error Analysis\n",
    "\n",
    "Now let's actually look at the failures. This is where you learn something."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create error view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = dataset.match(F(\"is_correct\") == False)\n",
    "print(f\"Total errors: {len(errors)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Launch App focused on errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = fo.launch_app(errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In the App, explore:**\n",
    "- What do the failed samples look like?\n",
    "- Are there patterns? (e.g., low contrast images, unusual anatomy)\n",
    "- Does the model consistently fail on certain question phrasings?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the worst slices\n",
    "\n",
    "Which combinations of (modality, body_part, content_type) have the worst accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "slice_stats = defaultdict(lambda: {\"correct\": 0, \"total\": 0})\n",
    "\n",
    "for sample in dataset.iter_samples():\n",
    "    key = (sample.modality, sample.body_part, sample.content_type)\n",
    "    slice_stats[key][\"total\"] += 1\n",
    "    if sample.is_correct:\n",
    "        slice_stats[key][\"correct\"] += 1\n",
    "\n",
    "# Find worst performing slices (min 10 samples)\n",
    "worst_slices = []\n",
    "for key, stats in slice_stats.items():\n",
    "    if stats[\"total\"] >= 10:\n",
    "        acc = stats[\"correct\"] / stats[\"total\"]\n",
    "        worst_slices.append((key, acc, stats[\"total\"]))\n",
    "\n",
    "print(\"Worst performing slices (n >= 10):\")\n",
    "for (modality, body_part, content_type), acc, n in sorted(worst_slices, key=lambda x: x[1])[:10]:\n",
    "    print(f\"  {modality} / {body_part} / {content_type}: {acc:.2%} (n={n})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tag error patterns\n",
    "\n",
    "As you review errors, tag them with patterns you notice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Tag errors where model gave opposite answer for yes/no questions\n",
    "yes_no_errors = errors.match(\n",
    "    (F(\"answer\").is_in([\"yes\", \"no\", \"Yes\", \"No\"])) & \n",
    "    (F(\"medgemma_answer.label\").is_in([\"yes\", \"no\", \"Yes\", \"No\"]))\n",
    ")\n",
    "\n",
    "for sample in yes_no_errors.iter_samples(autosave=True):\n",
    "    gt = sample.answer.lower()\n",
    "    pred = sample.medgemma_answer.label.lower() if sample.medgemma_answer else \"\"\n",
    "    if (gt == \"yes\" and pred == \"no\") or (gt == \"no\" and pred == \"yes\"):\n",
    "        sample.tags.append(\"opposite_answer\")\n",
    "\n",
    "print(f\"Tagged {len(dataset.match(F('tags').contains('opposite_answer')))} samples with 'opposite_answer'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find similar images to errors\n",
    "\n",
    "Use the similarity index to find images similar to failures. \n",
    "Are the similar images also failures? That suggests a systematic issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a specific error sample\n",
    "error_sample = errors.first()\n",
    "\n",
    "# Find similar images\n",
    "similar_view = sim_index.sort_by_similarity(error_sample.id, k=10)\n",
    "\n",
    "# Check how many of the similar images are also errors\n",
    "similar_errors = similar_view.match(F(\"is_correct\") == False)\n",
    "print(f\"Error sample: {error_sample.question}\")\n",
    "print(f\"Similar images that are also errors: {len(similar_errors)}/10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the similar images\n",
    "session = fo.launch_app(similar_view)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Fine-Tuning MedGemma for Localization\n",
    "\n",
    "You've explored the data, identified failure patterns, and have hypotheses about what to fix.\n",
    "Now let's fine-tune MedGemma to output bounding box coordinates for localization tasks.\n",
    "\n",
    "We'll follow these steps:\n",
    "1. Define a `GetItem` subclass to extract and transform data from FiftyOne\n",
    "2. Create train/val splits and flatten detections using `to_patches()`\n",
    "3. Set up QLoRA fine-tuning with the TRL library's `SFTTrainer`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install fine-tuning dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade --quiet bitsandbytes peft trl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Define the GetItem subclass\n",
    "\n",
    "FiftyOne's [`GetItem`](https://docs.voxel51.com/api/fiftyone.utils.torch.html#fiftyone.utils.torch.GetItem) \n",
    "class is the bridge between FiftyOne and PyTorch. It tells FiftyOne:\n",
    "1. What fields to extract from each sample (via `required_keys`)\n",
    "2. How to transform them into your desired format (via `__call__`)\n",
    "\n",
    "The `field_mapping` parameter is important when working with patches. In a patches view,\n",
    "the detection data lives in the original field name (e.g., \"detections\"), but we want \n",
    "to access it with a generic name in our code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "from PIL import Image\n",
    "from fiftyone.utils.torch import GetItem\n",
    "\n",
    "# System prompt for localization task\n",
    "LOCALIZATION_SYSTEM_PROMPT = \"\"\"Instructions:\n",
    "The following user query will require outputting bounding boxes. The format of bounding boxes coordinates is [y0, x0, y1, x1] where (y0, x0) must be top-left corner and (y1, x1) the bottom-right corner. This implies that x0 < x1 and y0 < y1. Always normalize the x and y coordinates the range [0, 1000], meaning that a bounding box starting at 15% of the image width would be associated with an x coordinate of 150. You MUST output a single parseable json list of objects enclosed into ```json...``` brackets, for instance ```json[{\"box_2d\": [800, 3, 840, 471], \"label\": \"car\"}, {\"box_2d\": [400, 22, 600, 73], \"label\": \"dog\"}]``` is a valid output. Now answer to the user query.\n",
    "\n",
    "Remember \"left\" refers to the patient's left side where the heart is and sometimes underneath an L in the upper right corner of the image.\"\"\"\n",
    "\n",
    "\n",
    "class LocalizationGetItem(GetItem):\n",
    "    \"\"\"\n",
    "    Extracts and transforms detection data for MedGemma localization fine-tuning.\n",
    "    \n",
    "    Each patch sample (after to_patches()) contains:\n",
    "    - filepath: path to the full image\n",
    "    - detection: the Detection object (bbox, label, etc.)\n",
    "    - metadata: image dimensions\n",
    "    \n",
    "    We transform this into MedGemma's expected message format with:\n",
    "    - System prompt explaining the bbox output format\n",
    "    - User message with the localization query\n",
    "    - Assistant message with the target bbox in JSON format\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, field_mapping=None):\n",
    "        super().__init__(field_mapping=field_mapping)\n",
    "    \n",
    "    @property\n",
    "    def required_keys(self):\n",
    "        # These are the keys we'll access in __call__.\n",
    "        # 'detection' is a virtual key that gets mapped to the real field\n",
    "        # via field_mapping. 'filepath' and 'metadata' are standard fields.\n",
    "        return [\"filepath\", \"detection\", \"metadata\"]\n",
    "    \n",
    "    def __call__(self, d):\n",
    "        \"\"\"\n",
    "        Transform a FiftyOne sample dict into MedGemma fine-tuning format.\n",
    "        \n",
    "        Converts FiftyOne bbox format [x, y, w, h] in [0,1] to \n",
    "        MedGemma format [y0, x0, y1, x1] normalized to [0, 1000].\n",
    "        \"\"\"\n",
    "        filepath = d[\"filepath\"]\n",
    "        detection = d[\"detection\"]\n",
    "        \n",
    "        # Get the label from the detection\n",
    "        label = detection.label\n",
    "        \n",
    "        # --- Bounding Box Conversion ---\n",
    "        # FiftyOne stores bboxes as [x, y, width, height] with values in [0, 1]\n",
    "        # MedGemma expects [y0, x0, y1, x1] normalized to [0, 1000]\n",
    "        rx, ry, rw, rh = detection.bounding_box\n",
    "        \n",
    "        # Convert to [y0, x0, y1, x1] format, scaled to [0, 1000]\n",
    "        x0 = int(rx * 1000)\n",
    "        y0 = int(ry * 1000)\n",
    "        x1 = int((rx + rw) * 1000)\n",
    "        y1 = int((ry + rh) * 1000)\n",
    "        \n",
    "        # Format as [y0, x0, y1, x1] per the prompt instructions\n",
    "        bbox_normalized = [y0, x0, y1, x1]\n",
    "        \n",
    "        # --- Construct Messages ---\n",
    "        # Format the target response as JSON\n",
    "        target_json = f'```json[{{\"box_2d\": {bbox_normalized}, \"label\": \"{label}\"}}]```'\n",
    "        \n",
    "        # Build the message payload in chat format\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"image\"},\n",
    "                    {\"type\": \"text\", \"text\": f\"{LOCALIZATION_SYSTEM_PROMPT}\\n\\nLocate the {label} in this medical image.\"},\n",
    "                ],\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": target_json},\n",
    "                ],\n",
    "            },\n",
    "        ]\n",
    "        \n",
    "        return {\n",
    "            \"filepath\": filepath,\n",
    "            \"image\": Image.open(filepath).convert(\"RGB\"),\n",
    "            \"messages\": messages,\n",
    "            \"label\": label,\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Create train/val split and flatten detections\n",
    "\n",
    "Since our dataset doesn't have existing train/val tags, we'll create them.\n",
    "Then we use `to_patches(\"detections\")` to flatten the dataset so each detection \n",
    "becomes its own sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone.utils.random as four\n",
    "\n",
    "# Filter to samples that have detections\n",
    "has_detections_view = dataset.match(F(\"detections\") != None)\n",
    "print(f\"Samples with detections: {len(has_detections_view)}\")\n",
    "\n",
    "# Create train/val split (80/20)\n",
    "four.random_split(has_detections_view, {\"train\": 0.8, \"val\": 0.2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter by split tags\n",
    "train_view = has_detections_view.match_tags(\"train\")\n",
    "val_view = has_detections_view.match_tags(\"val\")\n",
    "\n",
    "print(f\"Samples - train: {len(train_view)}, val: {len(val_view)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten using to_patches() - each detection becomes its own sample\n",
    "train_patches = train_view.to_patches(\"detections\")\n",
    "val_patches = val_view.to_patches(\"detections\")\n",
    "\n",
    "print(f\"Patches - train: {len(train_patches)}, val: {len(val_patches)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Convert to PyTorch datasets\n",
    "\n",
    "Use `to_torch()` with our `GetItem` class to create PyTorch-compatible datasets.\n",
    "The `field_mapping` tells FiftyOne which actual field to use when we access \"detection\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up field mapping - in patches view, each sample's detection data \n",
    "# lives in the original field \"detections\"\n",
    "field_mapping = {\"detection\": \"detections\"}\n",
    "\n",
    "# Create GetItem instances\n",
    "train_getter = LocalizationGetItem(field_mapping=field_mapping)\n",
    "val_getter = LocalizationGetItem(field_mapping=field_mapping)\n",
    "\n",
    "# Convert to PyTorch datasets\n",
    "train_dataset = train_patches.to_torch(train_getter)\n",
    "val_dataset = val_patches.to_torch(val_getter)\n",
    "\n",
    "print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "print(f\"Val dataset size: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the data format\n",
    "sample = train_dataset[0]\n",
    "print(\"Sample keys:\", sample.keys())\n",
    "print(\"Label:\", sample[\"label\"])\n",
    "print(\"Messages structure:\")\n",
    "for msg in sample[\"messages\"]:\n",
    "    print(f\"  Role: {msg['role']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Load MedGemma with QLoRA configuration\n",
    "\n",
    "We use 4-bit quantization (QLoRA) to reduce memory requirements while maintaining\n",
    "fine-tuning capability. This allows fine-tuning on consumer GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoProcessor, AutoModelForImageTextToText, BitsAndBytesConfig\n",
    "\n",
    "model_id = \"google/medgemma-1.5-4b-it\"\n",
    "\n",
    "# Check if GPU supports bfloat16\n",
    "if torch.cuda.is_available() and torch.cuda.get_device_capability()[0] < 8:\n",
    "    raise ValueError(\"GPU does not support bfloat16, please use a GPU that supports bfloat16.\")\n",
    "\n",
    "model_kwargs = dict(\n",
    "    attn_implementation=\"eager\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "# Configure 4-bit quantization for memory efficiency\n",
    "model_kwargs[\"quantization_config\"] = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=model_kwargs[\"torch_dtype\"],\n",
    "    bnb_4bit_quant_storage=model_kwargs[\"torch_dtype\"],\n",
    ")\n",
    "\n",
    "model = AutoModelForImageTextToText.from_pretrained(model_id, **model_kwargs)\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "# Use right padding during training to avoid issues\n",
    "processor.tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Configure LoRA\n",
    "\n",
    "LoRA (Low-Rank Adaptation) allows efficient fine-tuning by only training \n",
    "small adapter matrices instead of all model weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    r=16,\n",
    "    bias=\"none\",\n",
    "    target_modules=\"all-linear\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    modules_to_save=[\n",
    "        \"lm_head\",\n",
    "        \"embed_tokens\",\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Define the collate function\n",
    "\n",
    "The collate function processes batches by:\n",
    "1. Applying the chat template to format messages\n",
    "2. Processing images and text together\n",
    "3. Creating labels with proper masking for padding and image tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(examples: list[dict[str, Any]]):\n",
    "    texts = []\n",
    "    images = []\n",
    "    \n",
    "    for example in examples:\n",
    "        # Convert image to RGB and wrap in list (processor expects list of images per sample)\n",
    "        images.append([example[\"image\"].convert(\"RGB\")])\n",
    "        \n",
    "        # Apply chat template to format the conversation\n",
    "        texts.append(processor.apply_chat_template(\n",
    "            example[\"messages\"], \n",
    "            add_generation_prompt=False, \n",
    "            tokenize=False\n",
    "        ).strip())\n",
    "    \n",
    "    # Tokenize texts and process images\n",
    "    batch = processor(text=texts, images=images, return_tensors=\"pt\", padding=True)\n",
    "    \n",
    "    # Create labels from input_ids\n",
    "    # We mask padding tokens and image tokens so they don't contribute to loss\n",
    "    labels = batch[\"input_ids\"].clone()\n",
    "    \n",
    "    # Get the image token ID to mask it\n",
    "    image_token_id = processor.tokenizer.convert_tokens_to_ids(\n",
    "        processor.tokenizer.special_tokens_map[\"boi_token\"]\n",
    "    )\n",
    "    \n",
    "    # Mask tokens that should not be used in loss computation\n",
    "    labels[labels == processor.tokenizer.pad_token_id] = -100\n",
    "    labels[labels == image_token_id] = -100\n",
    "    labels[labels == 262144] = -100  # Additional image token ID\n",
    "    \n",
    "    batch[\"labels\"] = labels\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Configure training\n",
    "\n",
    "We use TRL's `SFTConfig` and `SFTTrainer` for a clean training setup with\n",
    "all the best practices built in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTConfig, SFTTrainer\n",
    "\n",
    "num_train_epochs = 1  # Adjust based on your needs\n",
    "learning_rate = 2e-4\n",
    "\n",
    "training_args = SFTConfig(\n",
    "    output_dir=\"medgemma-localization-lora\",         # Directory to save the model\n",
    "    num_train_epochs=num_train_epochs,               # Number of training epochs\n",
    "    per_device_train_batch_size=4,                   # Batch size per device during training\n",
    "    per_device_eval_batch_size=4,                    # Batch size per device during evaluation\n",
    "    gradient_accumulation_steps=4,                   # Number of steps before performing a backward/update pass\n",
    "    gradient_checkpointing=True,                     # Enable gradient checkpointing to reduce memory usage\n",
    "    optim=\"adamw_torch_fused\",                       # Use fused AdamW optimizer for better performance\n",
    "    logging_steps=50,                                # Number of steps between logs\n",
    "    save_strategy=\"epoch\",                           # Save checkpoint every epoch\n",
    "    eval_strategy=\"steps\",                           # Evaluate every `eval_steps`\n",
    "    eval_steps=50,                                   # Number of steps between evaluations\n",
    "    learning_rate=learning_rate,                     # Learning rate\n",
    "    bf16=True,                                       # Use bfloat16 precision\n",
    "    max_grad_norm=0.3,                               # Max gradient norm\n",
    "    warmup_ratio=0.03,                               # Warmup ratio\n",
    "    lr_scheduler_type=\"linear\",                      # Use linear learning rate scheduler\n",
    "    push_to_hub=False,                               # Set to True to push model to Hub\n",
    "    report_to=\"tensorboard\",                         # Report metrics to tensorboard\n",
    "    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
    "    dataset_kwargs={\"skip_prepare_dataset\": True},   # We preprocess manually\n",
    "    remove_unused_columns=False,                     # Keep columns for data collator\n",
    "    label_names=[\"labels\"],                          # Input keys that correspond to labels\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8: Create trainer and train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    peft_config=peft_config,\n",
    "    processing_class=processor,\n",
    "    data_collator=collate_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the fine-tuned model\n",
    "trainer.save_model()\n",
    "\n",
    "# Optional: Push to Hugging Face Hub\n",
    "# trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean up GPU memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "del trainer\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Evaluating Your Fine-Tuned Model\n",
    "\n",
    "Now that you've fine-tuned MedGemma for localization, go back through the earlier \n",
    "sections of this notebook to evaluate how well your model performs:\n",
    "\n",
    "1. **Load your fine-tuned model** and run inference on the validation set\n",
    "2. **Store predictions** in FiftyOne alongside the ground truth\n",
    "3. **Use the evaluation techniques** from Sections 5 and 6:\n",
    "   - Compute accuracy by modality, body part, and content type\n",
    "   - Analyze errors using the App and similarity search\n",
    "   - Tag patterns in failures\n",
    "\n",
    "This iterative workflowâ€”explore, model, evaluate, fine-tuneâ€”is how you systematically\n",
    "improve your model's performance on specific failure modes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Bringing It All Together\n",
    "\n",
    "Here's what you've learned to do:\n",
    "\n",
    "| Step | What You Did | Why It Matters |\n",
    "|------|-------------|----------------|\n",
    "| Load & Explore | Understood data distribution before modeling | Caught potential issues early |\n",
    "| Embeddings | Visualized MedSigLIP clusters | Diagnosed whether classes are separable |\n",
    "| Inference | Ran MedGemma, stored predictions with data | Everything in one place for analysis |\n",
    "| Evaluation | Sliced accuracy by modality, body part, etc. | Found *where* the model fails |\n",
    "| Error Analysis | Visualized failures, tagged patterns | Understood *why* it fails |\n",
    "| Fine-Tuning | Used GetItem + SFTTrainer for localization | Improved model on specific failure modes |\n",
    "\n",
    "**The workflow you built here works for any dataset, any model, any challenge.**\n",
    "\n",
    "Whether you're doing:\n",
    "- Chest X-ray report generation\n",
    "- Dermatology classification  \n",
    "- CT severity assessment\n",
    "- Histopathology analysis\n",
    "\n",
    "The pattern is the same:\n",
    "1. Organize your data in FiftyOne\n",
    "2. Understand it before modeling\n",
    "3. Run inference, store predictions\n",
    "4. Slice, visualize, debug\n",
    "5. Fine-tune and iterate\n",
    "\n",
    "**Now go win that challenge.** ðŸ†"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Resources\n",
    "\n",
    "- [FiftyOne Documentation](https://docs.voxel51.com/)\n",
    "- [FiftyOne PyTorch Integration](https://docs.voxel51.com/api/fiftyone.utils.torch.html)\n",
    "- [SLAKE Dataset on HuggingFace](https://huggingface.co/datasets/Voxel51/SLAKE)\n",
    "- [MedGemma Model Card](https://huggingface.co/google/medgemma-1.5-4b-it)\n",
    "- [MedSigLIP Model Card](https://huggingface.co/google/medsiglip-448)\n",
    "- [TRL SFTTrainer Documentation](https://huggingface.co/docs/trl/sft_trainer)\n",
    "- [PEFT LoRA Documentation](https://huggingface.co/docs/peft/main/en/conceptual_guides/lora)\n",
    "- [MedGemma Impact Challenge](https://www.kaggle.com/competitions/med-gemma-impact-challenge)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fiftyone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
